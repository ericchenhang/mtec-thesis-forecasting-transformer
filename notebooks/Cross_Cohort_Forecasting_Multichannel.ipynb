{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Cohort Transaction Sequence Forecasting on the Multichannel Dataset\n",
        "\n",
        "This notebook implements the complete forecasting pipeline described in the accompanying thesis. It is optimized for execution on a GPU to accelerate model training and inference.\n",
        "\n",
        "The workflow proceeds as follows:\n",
        "1. **Environment Setup**: Load libraries and define utility functions.\n",
        "2. **Data Preparation**: Load and preprocess the Multichannel dataset in accordance with the temporal framing and forecasting task.\n",
        "3. **Model Definition**: Specify the architecture of the Transformer-based forecasting model and supporting components.\n",
        "4. **Training & Evaluation**: Configure and execute the training loop, followed by performance evaluation and visualization of key results.\n",
        "5. **Execution Flow**: A main execution block is provided to orchestrate the training and prediction processes.\n",
        "\n",
        "Most configurations (e.g., model parameters, training horizon, evaluation metrics) are modular and adjustable. For details, please refer to the final section of the notebook.\n",
        "\n",
        "\n",
        "**Note**: This notebook expects the transaction dataset at `15-transactions_allCohorts.csv`. Please ensure this file is placed in the working directory.\n"
      ],
      "metadata": {
        "id": "PofLA9Dcqr47"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LayNOUoP-cgw"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybb5fao--eIu",
        "outputId": "3b21f3f2-13fd-4710-f678-289176930051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.distributions as dist\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# For metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDFRDSEs-g6c"
      },
      "source": [
        "## 2. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC104Fk7ILZB",
        "outputId": "8b01a7ef-5fd9-4480-dbc3-4691cbed0bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded: 190315 transactions\n"
          ]
        }
      ],
      "source": [
        "# Load the transaction data\n",
        "df = pd.read_csv('15-transactions_allCohorts.csv')\n",
        "print(f\"Data loaded: {df.shape[0]} transactions\")\n",
        "df.head(5)\n",
        "\n",
        "# Convert dates\n",
        "df['Date'] = pd.to_datetime(df['ORDER_DATE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKKk_f7YIMLR"
      },
      "outputs": [],
      "source": [
        "def prepare_sequence_data(df, customer_field='CUSTNO', date_field='Date', cohort_field='COHORT_NUMBER',\n",
        "                        cohort_range=None, train_periods=None, pred_periods=None):\n",
        "    \"\"\"\n",
        "    Prepare sequence data for transaction forecasting with flexible period and cohort configuration\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Transaction data\n",
        "    customer_field : str\n",
        "        Column name for customer ID\n",
        "    date_field : str\n",
        "        Column name for transaction date\n",
        "    cohort_field : str or None\n",
        "        Column name for cohort information\n",
        "    cohort_range : tuple or None\n",
        "        Range of cohorts to include (min_cohort, max_cohort)\n",
        "    train_periods : dict or None\n",
        "        Dictionary with 'input_start', 'input_end', 'target_start', 'target_end' for training\n",
        "    pred_periods : dict or None\n",
        "        Dictionary with 'input_start', 'input_end', 'target_start', 'target_end' for prediction\n",
        "    \"\"\"\n",
        "    # Convert date field to datetime if it's not already\n",
        "    df[date_field] = pd.to_datetime(df[date_field])\n",
        "\n",
        "    # Set default periods if not provided\n",
        "    if train_periods is None:\n",
        "        train_periods = {\n",
        "            'input_start': '2005-01-01',\n",
        "            'input_end': '2006-12-31',\n",
        "            'target_start': '2007-01-01',\n",
        "            'target_end': '2007-12-31'\n",
        "        }\n",
        "\n",
        "    if pred_periods is None:\n",
        "        pred_periods = {\n",
        "            'input_start': '2008-01-01',\n",
        "            'input_end': '2009-12-31',\n",
        "            'target_start': '2010-01-01',\n",
        "            'target_end': '2010-12-31'\n",
        "        }\n",
        "\n",
        "    # Extract periods\n",
        "    INPUT_START = train_periods['input_start']\n",
        "    INPUT_END = train_periods['input_end']\n",
        "    TARGET_START = train_periods['target_start']\n",
        "    TARGET_END = train_periods['target_end']\n",
        "\n",
        "    PRED_INPUT_START = pred_periods['input_start']\n",
        "    PRED_INPUT_END = pred_periods['input_end']\n",
        "    PRED_TARGET_START = pred_periods['target_start']\n",
        "    PRED_TARGET_END = pred_periods['target_end']\n",
        "\n",
        "    print(f\"Training input: {INPUT_START} to {INPUT_END}\")\n",
        "    print(f\"Training target: {TARGET_START} to {TARGET_END}\")\n",
        "    print(f\"Prediction input: {PRED_INPUT_START} to {PRED_INPUT_END}\")\n",
        "    print(f\"Prediction target: {PRED_TARGET_START} to {PRED_TARGET_END}\")\n",
        "\n",
        "    # Apply cohort filtering if specified\n",
        "    if cohort_range is not None and cohort_field is not None:\n",
        "        min_cohort, max_cohort = cohort_range\n",
        "\n",
        "        # Check data type of cohort_field and convert if necessary\n",
        "        cohort_dtype = df[cohort_field].dtype\n",
        "\n",
        "        if cohort_dtype == 'object' or cohort_dtype == 'string':\n",
        "            # If cohort is string/object type, convert range values to string\n",
        "            min_cohort_val = str(min_cohort)\n",
        "            max_cohort_val = str(max_cohort)\n",
        "        elif 'datetime' in str(cohort_dtype):\n",
        "            # If cohort is datetime, ensure range values are datetime\n",
        "            min_cohort_val = pd.to_datetime(min_cohort)\n",
        "            max_cohort_val = pd.to_datetime(max_cohort)\n",
        "        else:\n",
        "            # For numeric types, make sure the types match\n",
        "            if isinstance(min_cohort, str):\n",
        "                # If they're strings but should be numeric, convert the column\n",
        "                df[cohort_field] = pd.to_numeric(df[cohort_field], errors='coerce')\n",
        "                min_cohort_val = float(min_cohort)\n",
        "                max_cohort_val = float(max_cohort)\n",
        "            else:\n",
        "                # Otherwise use the values as-is\n",
        "                min_cohort_val = min_cohort\n",
        "                max_cohort_val = max_cohort\n",
        "\n",
        "        # Apply the filter with proper types\n",
        "        df_filtered = df[(df[cohort_field] >= min_cohort_val) & (df[cohort_field] <= max_cohort_val)]\n",
        "        print(f\"Filtered to cohorts {min_cohort}-{max_cohort}: {len(df_filtered[customer_field].unique())} customers\")\n",
        "    else:\n",
        "        df_filtered = df\n",
        "        print(f\"Using all cohorts: {len(df_filtered[customer_field].unique())} customers\")\n",
        "\n",
        "    # Create customer mapping\n",
        "    customer_ids = df_filtered[customer_field].unique()\n",
        "    customer_to_idx = {cid: idx for idx, cid in enumerate(customer_ids)}\n",
        "\n",
        "    # Create date ranges - use Monday as the start of the week for consistency\n",
        "    input_dates = pd.date_range(INPUT_START, INPUT_END, freq='W-MON')\n",
        "    target_dates = pd.date_range(TARGET_START, TARGET_END, freq='W-MON')\n",
        "    pred_input_dates = pd.date_range(PRED_INPUT_START, PRED_INPUT_END, freq='W-MON')\n",
        "    pred_target_dates = pd.date_range(PRED_TARGET_START, PRED_TARGET_END, freq='W-MON')\n",
        "\n",
        "    print(f\"Training input weeks: {len(input_dates)}\")\n",
        "    print(f\"Training target weeks: {len(target_dates)}\")\n",
        "    print(f\"Prediction input weeks: {len(pred_input_dates)}\")\n",
        "    print(f\"Prediction target weeks: {len(pred_target_dates)}\")\n",
        "\n",
        "    # Prepare containers for data\n",
        "    X_train_data = []\n",
        "    y_target_data = []\n",
        "    X_pred_data = []\n",
        "    y_pred_target_data = []\n",
        "    customer_indices = []\n",
        "    customer_cohorts = []\n",
        "\n",
        "    # Process each customer\n",
        "    for customer_id in tqdm(customer_ids, desc=\"Processing customers\"):\n",
        "        # Filter customer transactions\n",
        "        customer_df = df_filtered[df_filtered[customer_field] == customer_id].copy()\n",
        "\n",
        "        # Get customer cohort if available\n",
        "        if cohort_field is not None:\n",
        "            customer_cohort = customer_df[cohort_field].iloc[0]\n",
        "        else:\n",
        "            customer_cohort = 0\n",
        "\n",
        "        # Aggregate by week\n",
        "        weekly_counts = customer_df.groupby([pd.Grouper(key=date_field, freq='W-MON')]).size().to_frame('transactions')\n",
        "\n",
        "        # Create templates with all dates\n",
        "        train_input_template = pd.DataFrame(index=input_dates)\n",
        "        train_target_template = pd.DataFrame(index=target_dates)\n",
        "        pred_input_template = pd.DataFrame(index=pred_input_dates)\n",
        "        pred_target_template = pd.DataFrame(index=pred_target_dates)\n",
        "\n",
        "        # Merge transaction counts\n",
        "        train_input_data = train_input_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "        train_target_data = train_target_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "        pred_input_data = pred_input_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "        pred_target_data = pred_target_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "\n",
        "        # Add to datasets\n",
        "        X_train_data.append(train_input_data)\n",
        "        y_target_data.append(train_target_data)\n",
        "        X_pred_data.append(pred_input_data)\n",
        "        y_pred_target_data.append(pred_target_data)\n",
        "\n",
        "        # Add customer index and cohort\n",
        "        customer_indices.append(customer_to_idx[customer_id])\n",
        "        customer_cohorts.append(customer_cohort)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(np.array(X_train_data), dtype=torch.float32)\n",
        "    y_target_tensor = torch.tensor(np.array(y_target_data), dtype=torch.float32)\n",
        "    X_pred_tensor = torch.tensor(np.array(X_pred_data), dtype=torch.float32)\n",
        "    y_pred_target_tensor = torch.tensor(np.array(y_pred_target_data), dtype=torch.float32)\n",
        "    customer_indices_tensor = torch.tensor(customer_indices, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_tensor,\n",
        "        'y_target': y_target_tensor,\n",
        "        'X_pred': X_pred_tensor,\n",
        "        'y_pred_target': y_pred_target_tensor,\n",
        "        'customer_indices': customer_indices_tensor,\n",
        "        'customer_mapping': customer_to_idx,\n",
        "        'num_customers': len(customer_ids),\n",
        "        'customer_cohorts': customer_cohorts,\n",
        "        'train_periods': train_periods,\n",
        "        'pred_periods': pred_periods,\n",
        "        'input_dates': input_dates,\n",
        "        'target_dates': target_dates,\n",
        "        'pred_input_dates': pred_input_dates,\n",
        "        'pred_target_dates': pred_target_dates\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W30pW91i-xn5"
      },
      "outputs": [],
      "source": [
        "def prepare_cross_customer_validation(data, validation_ratio=0.1, random_seed=None):\n",
        "    \"\"\"\n",
        "    Split data by customers for validation\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : dict\n",
        "        Dictionary containing 'X_train', 'y_target', and 'customer_indices'\n",
        "    validation_ratio : float\n",
        "        Proportion of customers to use for validation (default: 0.1)\n",
        "    random_seed : int, optional\n",
        "        Random seed for reproducibility (default: None)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing train/validation splits\n",
        "    \"\"\"\n",
        "    # Extract components\n",
        "    X_train = data['X_train']\n",
        "    y_target = data['y_target']\n",
        "    customer_indices = data['customer_indices']\n",
        "\n",
        "    # Get number of customers\n",
        "    num_customers = len(X_train)\n",
        "    num_val = int(num_customers * validation_ratio)\n",
        "\n",
        "    # Set random seed if provided\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # Random indices for validation\n",
        "    all_indices = np.arange(num_customers)\n",
        "    np.random.shuffle(all_indices)\n",
        "    val_indices = all_indices[:num_val]\n",
        "    train_indices = all_indices[num_val:]\n",
        "\n",
        "    # Create train and validation sets\n",
        "    X_train_split = X_train[train_indices]\n",
        "    y_train_split = y_target[train_indices]\n",
        "    customer_train = customer_indices[train_indices]\n",
        "\n",
        "    X_val_split = X_train[val_indices]\n",
        "    y_val_split = y_target[val_indices]\n",
        "    customer_val = customer_indices[val_indices]\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_split,\n",
        "        'y_train': y_train_split,\n",
        "        'customer_train': customer_train,\n",
        "        'X_val': X_val_split,\n",
        "        'y_val': y_val_split,\n",
        "        'customer_val': customer_val,\n",
        "        'train_indices': train_indices,\n",
        "        'val_indices': val_indices\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKuCQU2o-1h8"
      },
      "source": [
        "## 3. Dataset and Model Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJHHHyV7-3XL"
      },
      "outputs": [],
      "source": [
        "class CustomerTransactionDataset(Dataset):\n",
        "    \"\"\"Dataset for customer transaction data\"\"\"\n",
        "    def __init__(self, X, y, customer_ids):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.customer_ids = customer_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.customer_ids[idx]\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)  # Store on same device as model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(1), :].to(x.device)\n",
        "\n",
        "\n",
        "class TransactionAwareTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, num_heads, output_dim, num_customers=None, dropout=0.1):\n",
        "        super(TransactionAwareTransformer, self).__init__()\n",
        "\n",
        "        # Standard components\n",
        "        self.input_projection = nn.Linear(input_dim, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
        "        self.customer_embedding = nn.Embedding(num_customers+1, embed_dim)\n",
        "\n",
        "        # Transaction event detector\n",
        "        self.transaction_detector = nn.Sequential(\n",
        "            nn.Linear(input_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softplus()  # Ensure non-negative predictions\n",
        "        )\n",
        "\n",
        "        # Scale factor for prediction (learnable)\n",
        "        self.log_output_scale = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x, customer_ids, time_indices=None):\n",
        "        # Ensure correct dimensions\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(-1)\n",
        "\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # Transaction mask - highlight where transactions occurred\n",
        "        transaction_mask = (x > 0).float()\n",
        "\n",
        "        # Customer embedding\n",
        "        cust_embed = self.customer_embedding(customer_ids).unsqueeze(1).expand(-1, seq_len, -1)\n",
        "\n",
        "        # Process input\n",
        "        x_embed = self.input_projection(x)\n",
        "        x_embed = self.pos_encoding(x_embed)\n",
        "\n",
        "        # Add transaction signal\n",
        "        trans_signal = self.transaction_detector(x) * transaction_mask\n",
        "        x_embed = x_embed + cust_embed + trans_signal * 0.5\n",
        "\n",
        "        # Add time information if available\n",
        "        if time_indices is not None:\n",
        "            # Handle time indices shape\n",
        "            if time_indices.dim() == 1:\n",
        "                if time_indices.size(0) != seq_len:\n",
        "                    time_indices = time_indices[:seq_len] if time_indices.size(0) > seq_len else torch.cat([\n",
        "                        time_indices,\n",
        "                        time_indices[-1].repeat(seq_len - time_indices.size(0))\n",
        "                    ])\n",
        "                time_indices = time_indices.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "            # Use week of year as a feature (1-52)\n",
        "            week_of_year = (time_indices % 52) + 1\n",
        "            week_embed = torch.zeros((batch_size, seq_len, x_embed.size(-1)), device=x_embed.device)\n",
        "\n",
        "            # Create simple encoding - add a small signal based on week of year\n",
        "            for i in range(batch_size):\n",
        "                for j in range(seq_len):\n",
        "                    week = week_of_year[i, j].item()\n",
        "                    # Add signal for holiday seasons (weeks 50-52)\n",
        "                    if week >= 50:\n",
        "                        week_embed[i, j] += 0.2\n",
        "                    # Add signal for mid-year (weeks 25-27)\n",
        "                    elif 25 <= week <= 27:\n",
        "                        week_embed[i, j] += 0.1\n",
        "\n",
        "            x_embed = x_embed + week_embed\n",
        "\n",
        "        # Apply transformer\n",
        "        x_embed = self.transformer_encoder(x_embed)\n",
        "\n",
        "        # Use the entire context for prediction (average pooling)\n",
        "        # instead of just the final hidden state\n",
        "        final_hidden = x_embed.mean(dim=1)  # Average pooling over sequence length\n",
        "\n",
        "        # Generate and scale output\n",
        "        output = self.output_projection(final_hidden)\n",
        "        output = output * torch.exp(self.log_output_scale)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD9Kx6vj_L-s"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2wmn7Tv_NX2"
      },
      "outputs": [],
      "source": [
        "def simplified_transaction_forecaster_train(model, train_loader, val_loader, time_indices=None,\n",
        "                           num_epochs=50, patience=10, learning_rate=0.001, weight_decay=0.001, device=None):\n",
        "    \"\"\"\n",
        "    Training function for transaction sequence forecasting with\n",
        "    focus on handling imbalanced data and preventing zero predictions.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : nn.Module\n",
        "        Transaction aware transformer model\n",
        "    train_loader : DataLoader\n",
        "        DataLoader for training data\n",
        "    val_loader : DataLoader\n",
        "        DataLoader for validation data\n",
        "    time_indices : torch.Tensor, optional\n",
        "        Time indices for temporal information\n",
        "    num_epochs : int\n",
        "        Number of epochs to train\n",
        "    patience : int\n",
        "        Number of epochs to wait for improvement before early stopping\n",
        "    learning_rate : float\n",
        "        Learning rate for optimizer\n",
        "    weight_decay : float\n",
        "        Weight decay for optimizer\n",
        "    device : torch.device\n",
        "        Device to use for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : nn.Module\n",
        "        Trained model\n",
        "    dict\n",
        "        Training history\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 1. Initialization to prevent zero predictions\n",
        "    with torch.no_grad():\n",
        "        # Set output bias to positive values\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Linear) and 'output_projection' in name:\n",
        "                if hasattr(model, 'output_projection'):\n",
        "                    if module == model.output_projection[-2]:  # Final layer before activation\n",
        "                        module.bias.fill_(0.2)\n",
        "                        print(f\"Set positive bias in output layer\")\n",
        "\n",
        "        # Set log_output_scale to positive value\n",
        "        model.log_output_scale.fill_(1.0)  # Stronger initial value\n",
        "        print(f\"Set log_output_scale to {model.log_output_scale.item():.2f}\")\n",
        "\n",
        "    # Move time indices to device if provided\n",
        "    if time_indices is not None:\n",
        "        time_indices = time_indices.to(device)\n",
        "\n",
        "    # 2. Analyze data imbalance\n",
        "    print(\"Analyzing transaction data statistics...\")\n",
        "    total_elements = 0\n",
        "    nonzero_elements = 0\n",
        "    target_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, y_batch, _ in val_loader:\n",
        "            y_np = y_batch.numpy()\n",
        "            total_elements += y_np.size\n",
        "            nonzero_elements += np.sum(y_np > 0)\n",
        "            target_sum += np.sum(y_np)\n",
        "\n",
        "    # Calculate key statistics\n",
        "    tx_ratio = nonzero_elements / total_elements if total_elements > 0 else 0.01\n",
        "    target_mean = target_sum / total_elements if total_elements > 0 else 0.01\n",
        "\n",
        "    print(f\"Transaction ratio: {tx_ratio:.6f} ({nonzero_elements}/{total_elements})\")\n",
        "    print(f\"Target mean: {target_mean:.6f}\")\n",
        "\n",
        "    # Calculate transaction weight based on imbalance\n",
        "    tx_weight = min(max(5.0, 1.0 / (tx_ratio + 1e-8)), 100.0)\n",
        "    print(f\"Using transaction weight: {tx_weight:.2f}\")\n",
        "\n",
        "    # 3. Setup optimizer with different learning rates\n",
        "    output_params = []\n",
        "    other_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'output' in name or name == 'log_output_scale':\n",
        "            output_params.append(param)\n",
        "        else:\n",
        "            other_params.append(param)\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': other_params, 'lr': learning_rate},\n",
        "        {'params': output_params, 'lr': learning_rate * 0.5}  # Lower LR for output layers\n",
        "    ], weight_decay=weight_decay)\n",
        "\n",
        "    # LR scheduler with warmup\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=[learning_rate, learning_rate * 0.5],\n",
        "        total_steps=num_epochs * len(train_loader),\n",
        "        pct_start=0.2,  # 20% warmup\n",
        "        div_factor=20\n",
        "    )\n",
        "\n",
        "    # 4. Training state variables\n",
        "    best_val_metric = float('inf')\n",
        "    best_model = None\n",
        "    patience_counter = 0\n",
        "    best_scale_factor = 1.0\n",
        "    zero_prediction_counter = 0\n",
        "\n",
        "    # For mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_rmse': [],\n",
        "        'transaction_rmse': [],\n",
        "        'scale_factors': [],\n",
        "        'pred_means': []\n",
        "    }\n",
        "\n",
        "    # 5. Loss function with imbalance handling\n",
        "    def imbalanced_loss(y_pred, y_true, epoch):\n",
        "        \"\"\"Loss function optimized for imbalanced transaction data\"\"\"\n",
        "        # Basic stats\n",
        "        batch_mean_true = torch.mean(y_true)\n",
        "        batch_mean_pred = torch.mean(y_pred)\n",
        "\n",
        "        # Scale correction\n",
        "        if batch_mean_pred < 0.001:\n",
        "            scale_factor = torch.tensor(10.0, device=y_pred.device)\n",
        "        else:\n",
        "            scale_factor = batch_mean_true / (batch_mean_pred + 1e-8)\n",
        "            scale_factor = torch.clamp(scale_factor, 0.2, 50.0)\n",
        "\n",
        "        y_pred_scaled = y_pred * scale_factor\n",
        "\n",
        "        # Transaction masks\n",
        "        nonzero_mask = (y_true > 0)\n",
        "        zero_mask = ~nonzero_mask\n",
        "\n",
        "        # Weighted MSE with higher weights for transactions\n",
        "        weights = torch.ones_like(y_true, device=y_true.device)\n",
        "        if nonzero_mask.sum() > 0:\n",
        "            weights[nonzero_mask] = tx_weight\n",
        "\n",
        "            # Extra weight for high-value transactions\n",
        "            high_value_mask = (y_true > target_mean * 2)\n",
        "            if high_value_mask.sum() > 0:\n",
        "                weights[high_value_mask] *= 1.5\n",
        "\n",
        "        # Base loss with weights\n",
        "        base_loss = F.mse_loss(y_pred_scaled, y_true, reduction='none')\n",
        "        weighted_mse = (base_loss * weights).mean()\n",
        "\n",
        "        # Transaction-specific component\n",
        "        if nonzero_mask.sum() > 0:\n",
        "            # Transaction accuracy\n",
        "            tx_loss = F.mse_loss(y_pred_scaled[nonzero_mask], y_true[nonzero_mask])\n",
        "\n",
        "            # Zero-prediction penalty\n",
        "            zero_pred_mask = (y_pred_scaled[nonzero_mask] < 0.05)\n",
        "            if zero_pred_mask.sum() > 0:\n",
        "                zero_penalty = F.mse_loss(\n",
        "                    y_pred_scaled[nonzero_mask][zero_pred_mask],\n",
        "                    torch.ones_like(y_pred_scaled[nonzero_mask][zero_pred_mask]) * 0.2\n",
        "                ) * 10.0\n",
        "            else:\n",
        "                zero_penalty = torch.tensor(0.0, device=y_pred.device)\n",
        "        else:\n",
        "            tx_loss = torch.tensor(0.0, device=y_pred.device)\n",
        "            zero_penalty = torch.tensor(0.0, device=y_pred.device)\n",
        "\n",
        "        # Volume preservation\n",
        "        pred_sum = torch.sum(y_pred_scaled, dim=1)\n",
        "        true_sum = torch.sum(y_true, dim=1)\n",
        "        volume_penalty = torch.mean(torch.abs(pred_sum - true_sum))\n",
        "\n",
        "        # Zero-prediction trap escape\n",
        "        if batch_mean_pred < 0.01:\n",
        "            zero_trap_penalty = torch.exp(-100.0 * batch_mean_pred) * 10.0\n",
        "        else:\n",
        "            zero_trap_penalty = torch.tensor(0.0, device=y_pred.device)\n",
        "\n",
        "        # Early epochs: focus on non-zero predictions\n",
        "        if epoch < 5:\n",
        "            return weighted_mse + 2.0 * tx_loss + volume_penalty + zero_penalty + zero_trap_penalty\n",
        "        else:\n",
        "            return weighted_mse + 1.5 * tx_loss + volume_penalty + zero_penalty * 0.5\n",
        "\n",
        "    # 6. Training loop\n",
        "    print(f\"Starting transaction forecaster training on {device}...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TRAINING\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        epoch_scale_factors = []\n",
        "        epoch_pred_means = []\n",
        "\n",
        "        for X_batch, y_batch, customer_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            # Move data to device\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            customer_batch = customer_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward with mixed precision\n",
        "                with autocast():\n",
        "                    # Get predictions\n",
        "                    if time_indices is not None:\n",
        "                        predictions = model(X_batch, customer_batch, time_indices)\n",
        "                    else:\n",
        "                        predictions = model(X_batch, customer_batch)\n",
        "\n",
        "                    # Track statistics\n",
        "                    batch_mean_pred = torch.mean(predictions).item()\n",
        "                    epoch_pred_means.append(batch_mean_pred)\n",
        "\n",
        "                    # Handle zero predictions with offset if needed\n",
        "                    if batch_mean_pred < 0.001:\n",
        "                        predictions = predictions + 0.01\n",
        "                        batch_mean_pred = torch.mean(predictions).item()\n",
        "\n",
        "                    # Calculate scale factor\n",
        "                    batch_mean_true = torch.mean(y_batch).item()\n",
        "                    if batch_mean_pred > 1e-8:\n",
        "                        scale = batch_mean_true / batch_mean_pred\n",
        "                        scale = min(max(scale, 0.1), 100.0)\n",
        "                        epoch_scale_factors.append(scale)\n",
        "\n",
        "                    # Calculate imbalanced loss\n",
        "                    loss = imbalanced_loss(predictions, y_batch, epoch)\n",
        "\n",
        "                # Backward with scaling\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "                # Update weights\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                # Update LR\n",
        "                scheduler.step()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training step: {e}\")\n",
        "                # Fall back to simple MSE\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(X_batch, customer_batch)\n",
        "                simple_loss = F.mse_loss(predictions, y_batch)\n",
        "                simple_loss.backward()\n",
        "                optimizer.step()\n",
        "                loss = simple_loss\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Process epoch results\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Process prediction means\n",
        "        avg_pred_mean = np.mean(epoch_pred_means) if epoch_pred_means else 0\n",
        "        history['pred_means'].append(avg_pred_mean)\n",
        "        print(f\"  Average prediction mean: {avg_pred_mean:.6f}\")\n",
        "\n",
        "        # Process scale factors\n",
        "        if epoch_scale_factors:\n",
        "            avg_scale_factor = np.mean(epoch_scale_factors)\n",
        "            history['scale_factors'].append(avg_scale_factor)\n",
        "            print(f\"  Average scale factor: {avg_scale_factor:.2f}\")\n",
        "\n",
        "            # Update best scale factor\n",
        "            if avg_scale_factor > 0.5 and avg_scale_factor < 50.0:\n",
        "                if best_scale_factor == 1.0:\n",
        "                    best_scale_factor = avg_scale_factor\n",
        "                else:\n",
        "                    best_scale_factor = 0.8 * best_scale_factor + 0.2 * avg_scale_factor\n",
        "\n",
        "        # Handle zero predictions\n",
        "        if avg_pred_mean < 0.001:\n",
        "            zero_prediction_counter += 1\n",
        "            print(f\"  WARNING: Near-zero predictions detected ({zero_prediction_counter} epochs)\")\n",
        "\n",
        "            if zero_prediction_counter >= 2:\n",
        "                # Apply intervention\n",
        "                print(\"  Applying scale intervention\")\n",
        "                with torch.no_grad():\n",
        "                    # Boost log_output_scale\n",
        "                    current_scale = model.log_output_scale.item()\n",
        "                    model.log_output_scale.fill_(current_scale + 1.0)\n",
        "\n",
        "                    # Reset output bias\n",
        "                    for name, module in model.named_modules():\n",
        "                        if isinstance(module, nn.Linear) and 'output_projection' in name:\n",
        "                            if hasattr(model, 'output_projection'):\n",
        "                                if module == model.output_projection[-2]:\n",
        "                                    module.bias.fill_(0.5)\n",
        "        else:\n",
        "            zero_prediction_counter = 0\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, customer_batch in val_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                customer_batch = customer_batch.to(device)\n",
        "\n",
        "                # Get predictions\n",
        "                if time_indices is not None:\n",
        "                    val_preds = model(X_batch, customer_batch, time_indices)\n",
        "                else:\n",
        "                    val_preds = model(X_batch, customer_batch)\n",
        "\n",
        "                # Apply scale factor\n",
        "                scale_to_apply = best_scale_factor if best_scale_factor > 1.0 else 5.0\n",
        "                val_preds_scaled = val_preds * scale_to_apply\n",
        "\n",
        "                # Calculate simple loss\n",
        "                batch_loss = F.mse_loss(val_preds, y_batch).item()\n",
        "                val_loss += batch_loss\n",
        "\n",
        "                # Store predictions and targets\n",
        "                val_predictions.append(val_preds_scaled.cpu().numpy())\n",
        "                val_targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        val_predictions_np = np.vstack(val_predictions)\n",
        "        val_targets_np = np.vstack(val_targets)\n",
        "\n",
        "        val_mse = mean_squared_error(val_targets_np, val_predictions_np)\n",
        "        val_rmse = np.sqrt(val_mse)\n",
        "\n",
        "        # Transaction-specific metrics\n",
        "        nonzero_mask = val_targets_np > 0\n",
        "        if nonzero_mask.sum() > 0:\n",
        "            tx_mse = mean_squared_error(\n",
        "                val_targets_np[nonzero_mask],\n",
        "                val_predictions_np[nonzero_mask]\n",
        "            )\n",
        "            tx_rmse = np.sqrt(tx_mse)\n",
        "        else:\n",
        "            tx_rmse = 0.0\n",
        "\n",
        "        # Store metrics\n",
        "        val_loss /= len(val_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_rmse'].append(val_rmse)\n",
        "        history['transaction_rmse'].append(tx_rmse)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"  Validation RMSE: {val_rmse:.6f}, Transaction RMSE: {tx_rmse:.6f}\")\n",
        "\n",
        "        # Validation metric with focus on transaction accuracy\n",
        "        val_metric = val_rmse * 0.4 + tx_rmse * 0.6\n",
        "\n",
        "        # Check for improvement\n",
        "        if epoch >= 5 and avg_pred_mean > 0.0005 and val_metric < best_val_metric:\n",
        "            best_val_metric = val_metric\n",
        "            best_model = {k: v.cpu().detach() for k, v in model.state_dict().items()}\n",
        "            patience_counter = 0\n",
        "            print(f\"  New best model! Val metric: {val_metric:.6f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  No improvement for {patience_counter} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience and epoch >= 10:\n",
        "            print(f\"Early stopping after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model if available\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "        print(\"Loaded best model from checkpoint\")\n",
        "\n",
        "    # Store best scale factor\n",
        "    model.best_scale_factor = best_scale_factor\n",
        "    print(f\"Best scale factor: {best_scale_factor:.2f}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5z4PI-b_RWr"
      },
      "outputs": [],
      "source": [
        "def predict_and_evaluate(model, X_pred, y_target, customer_indices, time_indices=None, fixed_scale=14.0):\n",
        "    \"\"\"\n",
        "    Generate predictions and evaluate on target data with automatic scaling\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : nn.Module\n",
        "        Trained model\n",
        "    X_pred : torch.Tensor\n",
        "        Input data\n",
        "    y_target : torch.Tensor\n",
        "        Target values\n",
        "    customer_indices : torch.Tensor\n",
        "        Customer indices\n",
        "    time_indices : torch.Tensor, optional\n",
        "        Time indices for temporal information\n",
        "    fixed_scale : float, optional\n",
        "        Fixed scaling factor to apply to predictions (default: 14.0)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data\n",
        "    X_pred = X_pred.to(device)\n",
        "    y_target = y_target.to(device)\n",
        "    customer_indices = customer_indices.to(device)\n",
        "\n",
        "    if time_indices is not None:\n",
        "        time_indices = time_indices.to(device)\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        # Get raw predictions\n",
        "        raw_predictions = model(X_pred, customer_indices, time_indices)\n",
        "\n",
        "        # Apply fixed scaling for better results on new cohorts\n",
        "        predictions = raw_predictions * fixed_scale\n",
        "\n",
        "        # Log the scale used\n",
        "        print(f\"Applied fixed scale factor: {fixed_scale:.2f}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    predictions_np = predictions.cpu().numpy()\n",
        "    raw_predictions_np = raw_predictions.cpu().numpy()\n",
        "    targets_np = y_target.cpu().numpy()\n",
        "\n",
        "    # Overall metrics\n",
        "    mse = mean_squared_error(targets_np.flatten(), predictions_np.flatten())\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(targets_np.flatten(), predictions_np.flatten())\n",
        "\n",
        "    print(f\"Prediction Metrics:\")\n",
        "    print(f\"MSE: {mse:.6f}\")\n",
        "    print(f\"RMSE: {rmse:.6f}\")\n",
        "    print(f\"MAE: {mae:.6f}\")\n",
        "\n",
        "    # Transaction-specific metrics\n",
        "    nonzero_mask = targets_np > 0\n",
        "    if nonzero_mask.sum() > 0:\n",
        "        trans_mse = mean_squared_error(\n",
        "            targets_np[nonzero_mask],\n",
        "            predictions_np[nonzero_mask]\n",
        "        )\n",
        "        trans_rmse = np.sqrt(trans_mse)\n",
        "        trans_mae = mean_absolute_error(\n",
        "            targets_np[nonzero_mask],\n",
        "            predictions_np[nonzero_mask]\n",
        "        )\n",
        "\n",
        "        # Calculate transaction recall and precision\n",
        "        pred_threshold = 0.1  # Threshold to consider a prediction as significant\n",
        "        detected = (predictions_np[nonzero_mask] >= pred_threshold).sum()\n",
        "        total_transactions = nonzero_mask.sum()\n",
        "        tx_recall = detected / total_transactions if total_transactions > 0 else 0\n",
        "\n",
        "        # How many predicted transactions were actual transactions\n",
        "        predicted_tx_mask = predictions_np >= pred_threshold\n",
        "        true_positives = (predicted_tx_mask & nonzero_mask).sum()\n",
        "        tx_precision = true_positives / predicted_tx_mask.sum() if predicted_tx_mask.sum() > 0 else 0\n",
        "\n",
        "        print(f\"Transaction-only Metrics:\")\n",
        "        print(f\"Trans MSE: {trans_mse:.6f}\")\n",
        "        print(f\"Trans RMSE: {trans_rmse:.6f}\")\n",
        "        print(f\"Trans MAE: {trans_mae:.6f}\")\n",
        "        print(f\"Transaction Recall: {tx_recall:.4f} ({detected}/{total_transactions})\")\n",
        "        print(f\"Transaction Precision: {tx_precision:.4f}\")\n",
        "    else:\n",
        "        trans_mse = trans_rmse = trans_mae = tx_recall = tx_precision = None\n",
        "\n",
        "    # Volume metrics\n",
        "    pred_volume = np.sum(predictions_np)\n",
        "    raw_pred_volume = np.sum(raw_predictions_np)\n",
        "    target_volume = np.sum(targets_np)\n",
        "    volume_ratio = pred_volume / target_volume if target_volume > 0 else 0\n",
        "    raw_volume_ratio = raw_pred_volume / target_volume if target_volume > 0 else 0\n",
        "\n",
        "    print(f\"Volume Metrics:\")\n",
        "    print(f\"Predicted total transactions: {pred_volume:.1f}\")\n",
        "    print(f\"Raw (unscaled) predicted total: {raw_pred_volume:.1f}\")\n",
        "    print(f\"Actual total transactions: {target_volume:.1f}\")\n",
        "    print(f\"Volume ratio: {volume_ratio:.4f} (Raw: {raw_volume_ratio:.4f})\")\n",
        "\n",
        "    return predictions.cpu(), {\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'predictions': predictions_np,\n",
        "        'raw_predictions': raw_predictions_np,\n",
        "        'targets': targets_np,\n",
        "        'trans_mse': trans_mse,\n",
        "        'trans_rmse': trans_rmse,\n",
        "        'trans_mae': trans_mae,\n",
        "        'tx_recall': tx_recall,\n",
        "        'tx_precision': tx_precision,\n",
        "        'volume_ratio': volume_ratio\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnmwn8jE_g-6"
      },
      "source": [
        "## 6. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNod9SIK_m15"
      },
      "outputs": [],
      "source": [
        "def visualize_results(X_input, y_true, predictions, num_samples=5, title=\"Model Predictions\"):\n",
        "    \"\"\"\n",
        "    Visualize input sequences and predictions for selected customers\n",
        "    \"\"\"\n",
        "    # Select random samples\n",
        "    if num_samples > len(X_input):\n",
        "        num_samples = len(X_input)\n",
        "\n",
        "    indices = np.random.choice(len(X_input), num_samples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 4 * num_samples))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(num_samples, 1, i+1)\n",
        "\n",
        "        # Get data\n",
        "        input_seq = X_input[idx].numpy()\n",
        "        true_seq = y_true[idx].numpy()\n",
        "        pred_seq = predictions[idx].numpy()\n",
        "\n",
        "        # Create time indices\n",
        "        input_weeks = np.arange(len(input_seq))\n",
        "        output_weeks = np.arange(len(input_seq), len(input_seq) + len(true_seq))\n",
        "\n",
        "        # Plot\n",
        "        plt.plot(input_weeks, input_seq, 'o-', color='blue',\n",
        "                 label='Input Sequence', alpha=0.7, markersize=3)\n",
        "        plt.plot(output_weeks, true_seq, 'o-', color='green',\n",
        "                 label='True Target', alpha=0.7, markersize=3)\n",
        "        plt.plot(output_weeks, pred_seq, 'x--', color='red',\n",
        "                 label='Prediction', alpha=0.7, markersize=4)\n",
        "\n",
        "        # Add vertical line to separate input and output\n",
        "        plt.axvline(x=len(input_seq)-1, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.title(f'Customer {idx} Transaction Pattern')\n",
        "        plt.ylabel('Transactions')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_aggregated_results(X_input, y_true, predictions, title=\"Aggregated Model Predictions\"):\n",
        "    \"\"\"\n",
        "    Visualize aggregated predictions vs true values across all samples.\n",
        "    Shows average input, target, and predicted sequences over time.\n",
        "    \"\"\"\n",
        "    # Convert to NumPy arrays if necessary\n",
        "    if not isinstance(X_input, np.ndarray):\n",
        "        X_input = np.array([x.numpy() for x in X_input])\n",
        "    if not isinstance(y_true, np.ndarray):\n",
        "        y_true = np.array([y.numpy() for y in y_true])\n",
        "    if not isinstance(predictions, np.ndarray):\n",
        "        predictions = np.array([p.numpy() for p in predictions])\n",
        "\n",
        "    # Compute mean across samples\n",
        "    mean_input = X_input.mean(axis=0)\n",
        "    mean_true = y_true.mean(axis=0)\n",
        "    mean_pred = predictions.mean(axis=0)\n",
        "\n",
        "    # Time indices\n",
        "    input_weeks = np.arange(len(mean_input))\n",
        "    output_weeks = np.arange(len(mean_input), len(mean_input) + len(mean_true))\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    #plt.plot(input_weeks, mean_input, 'o-', label='Mean Input Sequence', color='blue')\n",
        "    plt.plot(output_weeks, mean_true, 'o-', label='Mean True Target', color='green')\n",
        "    plt.plot(output_weeks, mean_pred, 'x--', label='Mean Prediction', color='red')\n",
        "\n",
        "    plt.axvline(x=len(mean_input)-1, color='gray', linestyle='--', alpha=0.5)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Weeks')\n",
        "    plt.ylabel('Transaction Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training metrics history from the model training\"\"\"\n",
        "    plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot validation RMSE\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history['val_rmse'], label='Validation RMSE')\n",
        "    if 'transaction_rmse' in history:\n",
        "        plt.plot(history['transaction_rmse'], label='Transaction-only RMSE')\n",
        "    plt.title('RMSE During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot validation MAE if available\n",
        "    plt.subplot(2, 2, 3)\n",
        "    if 'val_mae' in history:\n",
        "        plt.plot(history['val_mae'], label='Validation MAE')\n",
        "        plt.title('MAE During Training')\n",
        "    else:\n",
        "        if 'pred_means' in history:\n",
        "            plt.plot(history['pred_means'], label='Prediction Means')\n",
        "            plt.title('Prediction Means During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot scale factors if available\n",
        "    plt.subplot(2, 2, 4)\n",
        "    if 'scale_factors' in history and history['scale_factors']:\n",
        "        plt.plot(history['scale_factors'], label='Scale Factors')\n",
        "        plt.title('Scale Factor Evolution')\n",
        "    else:\n",
        "        # Plot something else if scale factors not available\n",
        "        plt.plot(history['train_loss'], label='Training Loss Evolution')\n",
        "        plt.title('Loss Evolution')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQcjclzH_sDA"
      },
      "source": [
        "## 7. Main Execution Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RFFcfJI_y41"
      },
      "outputs": [],
      "source": [
        "# Define configurable time periods\n",
        "training_periods = {\n",
        "    'input_start': '2005-01-01',\n",
        "    'input_end': '2006-12-31',\n",
        "    'target_start': '2007-01-01',\n",
        "    'target_end': '2009-12-26'\n",
        "}\n",
        "\n",
        "prediction_periods = {\n",
        "    'input_start': '2007-01-01',\n",
        "    'input_end': '2008-12-28',\n",
        "    'target_start': '2009-01-01',\n",
        "    'target_end': '2011-12-31'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NduMpV8v_6Et"
      },
      "outputs": [],
      "source": [
        "# Prepare the transaction data\n",
        "df['Date'] = pd.to_datetime(df['ORDER_DATE'])\n",
        "\n",
        "# %%\n",
        "# Prepare data with configurable time periods and cohort ranges\n",
        "training_data = prepare_sequence_data(\n",
        "    df,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 24),  # For training cohorts 1-24\n",
        "    train_periods=training_periods,\n",
        "    pred_periods=prediction_periods\n",
        ")\n",
        "\n",
        "# Prepare prediction data for cohorts 37-60\n",
        "prediction_data = prepare_sequence_data(\n",
        "    df,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(25, 48),  # For prediction cohorts 25-48\n",
        "    train_periods=training_periods,\n",
        "    pred_periods=prediction_periods\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npv9p_C2JCC_"
      },
      "outputs": [],
      "source": [
        "# Extract training data\n",
        "X_train = training_data['X_train']\n",
        "y_train_target = training_data['y_target']\n",
        "train_customer_indices = training_data['customer_indices']\n",
        "train_num_customers = training_data['num_customers']\n",
        "\n",
        "# Extract prediction data\n",
        "X_pred = prediction_data['X_pred']\n",
        "y_pred_target = prediction_data['y_pred_target']\n",
        "pred_customer_indices = prediction_data['customer_indices']\n",
        "pred_num_customers = prediction_data['num_customers']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DVoW-R6JETi"
      },
      "outputs": [],
      "source": [
        "# Create time indices for weeks of year for both datasets\n",
        "training_input_start = pd.to_datetime(training_periods['input_start'])\n",
        "training_input_weeks = X_train.shape[1]\n",
        "training_time_indices = torch.tensor([\n",
        "    pd.to_datetime(training_input_start + pd.Timedelta(weeks=i)).isocalendar()[1]\n",
        "    for i in range(training_input_weeks)\n",
        "], dtype=torch.long)\n",
        "\n",
        "prediction_input_start = pd.to_datetime(prediction_periods['input_start'])\n",
        "prediction_input_weeks = X_pred.shape[1]\n",
        "prediction_time_indices = torch.tensor([\n",
        "    pd.to_datetime(prediction_input_start + pd.Timedelta(weeks=i)).isocalendar()[1]\n",
        "    for i in range(prediction_input_weeks)\n",
        "], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv3AnsSTJSMH"
      },
      "outputs": [],
      "source": [
        "# Create a reproducible split with seed 42\n",
        "validation_data = prepare_cross_customer_validation(\n",
        "    {\n",
        "        'X_train': X_train,\n",
        "        'y_target': y_train_target,\n",
        "        'customer_indices': train_customer_indices\n",
        "    },\n",
        "    validation_ratio=0.1,\n",
        "    random_seed=66\n",
        ")\n",
        "\n",
        "# Check average transactions - useful for model tuning\n",
        "print(f\"Average transactions in training data: {X_train.mean():.4f}\")\n",
        "print(f\"Average transactions in training target: {y_train_target.mean():.4f}\")\n",
        "print(f\"Average transactions in prediction data: {X_pred.mean():.4f}\")\n",
        "print(f\"Average transactions in prediction target: {y_pred_target.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv1TSze4Jb_2"
      },
      "outputs": [],
      "source": [
        "# Initialize model parameters\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': y_train_target.shape[1],\n",
        "    'num_customers': train_num_customers,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = TransactionAwareTransformer(**model_params).to(device)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataset = CustomerTransactionDataset(\n",
        "    validation_data['X_train'],\n",
        "    validation_data['y_train'],\n",
        "    validation_data['customer_train']\n",
        ")\n",
        "val_dataset = CustomerTransactionDataset(\n",
        "    validation_data['X_val'],\n",
        "    validation_data['y_val'],\n",
        "    validation_data['customer_val']\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xK6PaFRJhk7"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trained_model, training_history = simplified_transaction_forecaster_train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    time_indices=training_time_indices,\n",
        "    num_epochs=30,\n",
        "    patience=10,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=0.001,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "plot_training_history(training_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lAMS0hPBn1r"
      },
      "source": [
        "## 8. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP7iBeEQJkbO"
      },
      "outputs": [],
      "source": [
        "# For prediction on new cohorts, we need a new model that handles different customer indices\n",
        "# We'll use the weights from the trained model but adjust the customer embedding layer\n",
        "prediction_model_params = model_params.copy()\n",
        "prediction_model_params['num_customers'] = pred_num_customers\n",
        "prediction_model_params['output_dim'] = y_pred_target.shape[1]\n",
        "\n",
        "prediction_model = TransactionAwareTransformer(**prediction_model_params).to(device)\n",
        "\n",
        "# Copy weights except for customer embedding\n",
        "with torch.no_grad():\n",
        "    # Copy all parameters except customer embedding\n",
        "    for name, param in trained_model.named_parameters():\n",
        "        if 'customer_embedding' not in name:\n",
        "            corresponding_param = dict(prediction_model.named_parameters())[name]\n",
        "            if param.shape == corresponding_param.shape:\n",
        "                corresponding_param.copy_(param)\n",
        "            else:\n",
        "                print(f\"Warning: Shape mismatch for {name}, skipping...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq26mL-iJpNv"
      },
      "outputs": [],
      "source": [
        "# Evaluate on prediction set\n",
        "predictions, metrics = predict_and_evaluate(\n",
        "    prediction_model,\n",
        "    X_pred,\n",
        "    y_pred_target,\n",
        "    pred_customer_indices,\n",
        "    time_indices=prediction_time_indices,\n",
        "    fixed_scale=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri6Akh0QJtfR"
      },
      "outputs": [],
      "source": [
        "# Visualize individual results\n",
        "visualize_results(\n",
        "    X_pred,\n",
        "    y_pred_target,\n",
        "    predictions,\n",
        "    num_samples=10,\n",
        "    title=\"Transaction Predictions for Selected Customers\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJqW4OtUJyEr"
      },
      "outputs": [],
      "source": [
        "# Visualize aggregated results\n",
        "visualize_aggregated_results(\n",
        "    X_pred,\n",
        "    y_pred_target,\n",
        "    predictions,\n",
        "    title=\"Aggregated Transaction Predictions\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwMHnhCyJykW"
      },
      "source": [
        "##9. Alternative Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btljQlj2J3H1"
      },
      "outputs": [],
      "source": [
        "# Example of using different time periods\n",
        "short_period_config = {\n",
        "    'input_start': '2005-01-01',\n",
        "    'input_end': '2005-12-31',  # 1 year input\n",
        "    'target_start': '2006-01-01',\n",
        "    'target_end': '2006-06-30'  # 6 months target\n",
        "}\n",
        "\n",
        "short_prediction_config = {\n",
        "    'input_start': '2008-01-01',\n",
        "    'input_end': '2008-12-31',  # 1 year input\n",
        "    'target_start': '2009-01-01',\n",
        "    'target_end': '2009-06-30'  # 6 months target\n",
        "}\n",
        "\n",
        "# To use these configurations, run:\n",
        "\n",
        "# Prepare data with different time periods\n",
        "short_training_data = prepare_sequence_data(\n",
        "    df,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 24),\n",
        "    train_periods=short_period_config,\n",
        "    pred_periods=short_prediction_config\n",
        ")\n",
        "\n",
        "short_prediction_data = prepare_sequence_data(\n",
        "    df,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(37, 60),\n",
        "    train_periods=short_period_config,\n",
        "    pred_periods=short_prediction_config\n",
        ")\n",
        "\n",
        "\n",
        "# Example of different model configurations\n",
        "larger_model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 256,     # Increased embedding dimension\n",
        "    'hidden_dim': 1024,   # Increased hidden dimension\n",
        "    'num_layers': 4,      # More transformer layers\n",
        "    'num_heads': 16,      # More attention heads\n",
        "    'output_dim': y_train_target.shape[1],\n",
        "    'num_customers': train_num_customers,\n",
        "    'dropout': 0.2        # Increased dropout\n",
        "}\n",
        "\n",
        "# To use this configuration:\n",
        "\n",
        "# Initialize larger model\n",
        "larger_model = TransactionAwareTransformer(**larger_model_params).to(device)\n",
        "\n",
        "# Train the larger model\n",
        "larger_trained_model, larger_training_history = simplified_transaction_forecaster_train(\n",
        "    model=larger_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    time_indices=training_time_indices,\n",
        "    num_epochs=30,\n",
        "    patience=10,\n",
        "    learning_rate=0.0005,  # Lower learning rate for larger model\n",
        "    weight_decay=0.002,    # Increased weight decay\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# Example of different cohort ranges\n",
        "different_cohorts = {\n",
        "    'training': (5, 30),   # Different training cohorts\n",
        "    'prediction': (45, 70) # Different prediction cohorts\n",
        "}\n",
        "\n",
        "# To use these different cohort ranges:\n",
        "\n",
        "# Prepare data with different cohort ranges\n",
        "different_cohort_training_data = prepare_sequence_data(\n",
        "    df,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=different_cohorts['training'],\n",
        "    train_periods=training_periods,\n",
        "    pred_periods=prediction_periods\n",
        ")\n",
        "\n",
        "different_cohort_prediction_data = prepare_sequence_data(\n",
        "    df,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=different_cohorts['prediction'],\n",
        "    train_periods=training_periods,\n",
        "    pred_periods=prediction_periods\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
