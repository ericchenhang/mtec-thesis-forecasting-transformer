{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvUT9RptD7S8"
      },
      "source": [
        "## Sequence Sampling Forecasting on the Multichannel Dataset\n",
        "\n",
        "This notebook implements the sequence sampling forecasting pipeline introduced in the accompanying thesis. It is designed to operate on individual customer histories by segmenting their transaction records into overlapping input-output windows, thereby increasing temporal density and mitigating sparsity in the training data.\n",
        "\n",
        "The workflow proceeds as follows:\n",
        "1. **Environment Setup**: Load essential libraries and define utility functions.\n",
        "2. **Data Preparation**: Construct input-output pairs via sliding window sampling for each customer within the defined cohort and time ranges.\n",
        "3. **Model Definition**: Specify the Transformer-based forecasting model, incorporating customer-level embeddings and temporal features.\n",
        "4. **Training & Evaluation**: Train the model on the sampled sequences, validate performance using held-out segments, and visualize training dynamics.\n",
        "5. **Execution Flow**: A main execution block coordinates data preparation, model initialization, training, and evaluation.\n",
        "\n",
        "All key configurations—including sampling stride, window size, forecasting horizon, and model hyperparameters—are adjustable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4L2IYXmD7S-"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECYN5s6jD7S_",
        "outputId": "3bd5d8ed-07bd-4db9-9dea-ca36ee10548d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.distributions as dist\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# For metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# For metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX65waXyD7TA"
      },
      "source": [
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO2V52z59P9i"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowDataset(Dataset):\n",
        "    \"\"\"Dataset for sliding window transaction data\"\"\"\n",
        "    def __init__(self, X, y, customer_indices):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.customer_indices = customer_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.customer_indices[idx]\n",
        "\n",
        "def create_dataloaders(split_data, batch_size=32):\n",
        "    \"\"\"\n",
        "    Create DataLoaders for training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    split_data : dict\n",
        "        Dictionary containing train/val/test splits\n",
        "    batch_size : int\n",
        "        Batch size for DataLoaders\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing DataLoaders\n",
        "    \"\"\"\n",
        "    train_dataset = SlidingWindowDataset(\n",
        "        split_data['train']['X'],\n",
        "        split_data['train']['y'],\n",
        "        split_data['train']['customer_indices']\n",
        "    )\n",
        "\n",
        "    val_dataset = SlidingWindowDataset(\n",
        "        split_data['val']['X'],\n",
        "        split_data['val']['y'],\n",
        "        split_data['val']['customer_indices']\n",
        "    )\n",
        "\n",
        "    test_dataset = SlidingWindowDataset(\n",
        "        split_data['test']['X'],\n",
        "        split_data['test']['y'],\n",
        "        split_data['test']['customer_indices']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMVjXTzaiTOj"
      },
      "outputs": [],
      "source": [
        "def train_val_test_split(data, train_ratio=0.7, val_ratio=0.15, random_seed=42):\n",
        "    \"\"\"\n",
        "    Split data into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : dict\n",
        "        Dictionary containing 'X', 'y', and 'customer_indices'\n",
        "    train_ratio : float\n",
        "        Proportion of data to use for training\n",
        "    val_ratio : float\n",
        "        Proportion of data to use for validation\n",
        "    random_seed : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing train/val/test splits\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Get total number of sequences\n",
        "    n_sequences = len(data['X'])\n",
        "\n",
        "    # Generate random indices for shuffling\n",
        "    indices = np.random.permutation(n_sequences)\n",
        "\n",
        "    # Calculate split points\n",
        "    train_end = int(n_sequences * train_ratio)\n",
        "    val_end = int(n_sequences * (train_ratio + val_ratio))\n",
        "\n",
        "    # Split indices\n",
        "    train_indices = indices[:train_end]\n",
        "    val_indices = indices[train_end:val_end]\n",
        "    test_indices = indices[val_end:]\n",
        "\n",
        "    # Create splits\n",
        "    train_data = {\n",
        "        'X': data['X'][train_indices],\n",
        "        'y': data['y'][train_indices],\n",
        "        'customer_indices': data['customer_indices'][train_indices]\n",
        "    }\n",
        "\n",
        "    val_data = {\n",
        "        'X': data['X'][val_indices],\n",
        "        'y': data['y'][val_indices],\n",
        "        'customer_indices': data['customer_indices'][val_indices]\n",
        "    }\n",
        "\n",
        "    test_data = {\n",
        "        'X': data['X'][test_indices],\n",
        "        'y': data['y'][test_indices],\n",
        "        'customer_indices': data['customer_indices'][test_indices]\n",
        "    }\n",
        "\n",
        "    print(f\"Data split: {len(train_indices)} train, {len(val_indices)} validation, {len(test_indices)} test sequences\")\n",
        "\n",
        "    return {\n",
        "        'train': train_data,\n",
        "        'val': val_data,\n",
        "        'test': test_data,\n",
        "        'metadata': {\n",
        "            'train_ratio': train_ratio,\n",
        "            'val_ratio': val_ratio,\n",
        "            'test_ratio': 1 - train_ratio - val_ratio,\n",
        "            'random_seed': random_seed\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRhmk7KoHCRt"
      },
      "outputs": [],
      "source": [
        "def fixed_timeframe_sliding_window(df, training_input_period, training_target_period,\n",
        "                                  prediction_input_period, prediction_target_period,\n",
        "                                  window_size=16, step_size=4,\n",
        "                                  customer_field='CUSTNO', date_field='Date',\n",
        "                                  cohort_field=None, cohort_range=None):\n",
        "    \"\"\"\n",
        "    Prepare sequence data using sliding window approach with fixed time frames.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Transaction data\n",
        "    training_input_period : tuple\n",
        "        (start_date, end_date) for training input period\n",
        "    training_target_period : tuple\n",
        "        (start_date, end_date) for training target period\n",
        "    prediction_input_period : tuple\n",
        "        (start_date, end_date) for prediction input period\n",
        "    prediction_target_period : tuple\n",
        "        (start_date, end_date) for prediction target period\n",
        "    window_size : int\n",
        "        Size of input sequence window\n",
        "    step_size : int\n",
        "        Step size for sliding window (controls overlap)\n",
        "    customer_field : str\n",
        "        Column name for customer ID\n",
        "    date_field : str\n",
        "        Column name for transaction date\n",
        "    cohort_field : str or None\n",
        "        Column name for cohort information\n",
        "    cohort_range : tuple or None\n",
        "        Range of cohorts to include (min_cohort, max_cohort)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing processed data\n",
        "    \"\"\"\n",
        "    # Convert date periods to datetime\n",
        "    training_input_start, training_input_end = pd.to_datetime(training_input_period)\n",
        "    training_target_start, training_target_end = pd.to_datetime(training_target_period)\n",
        "    prediction_input_start, prediction_input_end = pd.to_datetime(prediction_input_period)\n",
        "    prediction_target_start, prediction_target_end = pd.to_datetime(prediction_target_period)\n",
        "\n",
        "    print(f\"Training input: {training_input_start} to {training_input_end}\")\n",
        "    print(f\"Training target: {training_target_start} to {training_target_end}\")\n",
        "    print(f\"Prediction input: {prediction_input_start} to {prediction_input_end}\")\n",
        "    print(f\"Prediction target: {prediction_target_start} to {prediction_target_end}\")\n",
        "\n",
        "    # Create date ranges\n",
        "    training_input_dates = pd.date_range(training_input_start, training_input_end, freq='W')\n",
        "    training_target_dates = pd.date_range(training_target_start, training_target_end, freq='W')\n",
        "    prediction_input_dates = pd.date_range(prediction_input_start, prediction_input_end, freq='W')\n",
        "    prediction_target_dates = pd.date_range(prediction_target_start, prediction_target_end, freq='W')\n",
        "\n",
        "    # Apply cohort filtering if specified\n",
        "    if cohort_range is not None and cohort_field is not None:\n",
        "        min_cohort, max_cohort = cohort_range\n",
        "\n",
        "        # Check data type of cohort_field and convert if necessary\n",
        "        cohort_dtype = df[cohort_field].dtype\n",
        "\n",
        "        if cohort_dtype == 'object' or cohort_dtype == 'string':\n",
        "            # If cohort is string/object type, convert range values to string\n",
        "            min_cohort_val = str(min_cohort)\n",
        "            max_cohort_val = str(max_cohort)\n",
        "        elif 'datetime' in str(cohort_dtype):\n",
        "            # If cohort is datetime, ensure range values are datetime\n",
        "            min_cohort_val = pd.to_datetime(min_cohort)\n",
        "            max_cohort_val = pd.to_datetime(max_cohort)\n",
        "        else:\n",
        "            # For numeric types\n",
        "            if isinstance(min_cohort, str):\n",
        "                # If they're strings but should be numeric, convert the column\n",
        "                df[cohort_field] = pd.to_numeric(df[cohort_field], errors='coerce')\n",
        "                min_cohort_val = float(min_cohort)\n",
        "                max_cohort_val = float(max_cohort)\n",
        "            else:\n",
        "                # Otherwise use the values as-is\n",
        "                min_cohort_val = min_cohort\n",
        "                max_cohort_val = max_cohort\n",
        "\n",
        "        # Apply the filter with proper types\n",
        "        df_filtered = df[(df[cohort_field] >= min_cohort_val) & (df[cohort_field] <= max_cohort_val)]\n",
        "        print(f\"Filtered to cohorts {min_cohort}-{max_cohort}: {len(df_filtered[customer_field].unique())} customers\")\n",
        "    else:\n",
        "        df_filtered = df\n",
        "        print(f\"Using all cohorts: {len(df_filtered[customer_field].unique())} customers\")\n",
        "\n",
        "    # Create customer mapping\n",
        "    customer_ids = df_filtered[customer_field].unique()\n",
        "    customer_to_idx = {cid: idx for idx, cid in enumerate(customer_ids)}\n",
        "\n",
        "    # Initialize lists to store data\n",
        "    X_train_windows = []\n",
        "    y_train_windows = []\n",
        "    X_pred_windows = []\n",
        "    y_pred_windows = []\n",
        "    customer_indices_train = []\n",
        "    customer_indices_pred = []\n",
        "    customer_cohorts = []\n",
        "\n",
        "    # Process each customer\n",
        "    train_valid_customers = 0\n",
        "    pred_valid_customers = 0\n",
        "    total_train_sequences = 0\n",
        "    total_pred_sequences = 0\n",
        "\n",
        "    print(f\"Processing {len(customer_ids)} customers with sliding window approach...\")\n",
        "\n",
        "    for customer_id in tqdm(customer_ids):\n",
        "        # Filter customer transactions\n",
        "        customer_df = df_filtered[df_filtered[customer_field] == customer_id].copy()\n",
        "\n",
        "        # Get customer cohort if available\n",
        "        if cohort_field is not None:\n",
        "            customer_cohort = customer_df[cohort_field].iloc[0]\n",
        "        else:\n",
        "            customer_cohort = 0\n",
        "\n",
        "        # Aggregate by week\n",
        "        weekly_counts = customer_df.groupby(pd.Grouper(key=date_field, freq='W')).size().to_frame('transactions')\n",
        "\n",
        "        # Create templates with all dates\n",
        "        train_input_template = pd.DataFrame(index=training_input_dates)\n",
        "        train_target_template = pd.DataFrame(index=training_target_dates)\n",
        "        pred_input_template = pd.DataFrame(index=prediction_input_dates)\n",
        "        pred_target_template = pd.DataFrame(index=prediction_target_dates)\n",
        "\n",
        "        # Merge transaction counts\n",
        "        train_input_data = train_input_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "        train_target_data = train_target_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "        pred_input_data = pred_input_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "        pred_target_data = pred_target_template.join(weekly_counts).fillna(0)['transactions'].values\n",
        "\n",
        "        # Training data - create sliding windows\n",
        "        if len(train_input_data) >= window_size:\n",
        "            train_valid_customers += 1\n",
        "\n",
        "            for start_idx in range(0, len(train_input_data) - window_size + 1, step_size):\n",
        "                end_idx = start_idx + window_size\n",
        "\n",
        "                # Ensure we don't go out of bounds\n",
        "                if end_idx <= len(train_input_data):\n",
        "                    X_window = train_input_data[start_idx:end_idx]\n",
        "                    y_window = train_target_data  # Use the entire target period\n",
        "\n",
        "                    X_train_windows.append(X_window)\n",
        "                    y_train_windows.append(y_window)\n",
        "                    customer_indices_train.append(customer_to_idx[customer_id])\n",
        "                    customer_cohorts.append(customer_cohort)\n",
        "                    total_train_sequences += 1\n",
        "\n",
        "        # Prediction data - create sliding windows\n",
        "        if len(pred_input_data) >= window_size:\n",
        "            pred_valid_customers += 1\n",
        "\n",
        "            for start_idx in range(0, len(pred_input_data) - window_size + 1, step_size):\n",
        "                end_idx = start_idx + window_size\n",
        "\n",
        "                # Ensure we don't go out of bounds\n",
        "                if end_idx <= len(pred_input_data):\n",
        "                    X_window = pred_input_data[start_idx:end_idx]\n",
        "                    y_window = pred_target_data  # Use the entire target period\n",
        "\n",
        "                    X_pred_windows.append(X_window)\n",
        "                    y_pred_windows.append(y_window)\n",
        "                    customer_indices_pred.append(customer_to_idx[customer_id])\n",
        "                    total_pred_sequences += 1\n",
        "\n",
        "    print(f\"Created {total_train_sequences} train sequences from {train_valid_customers} customers\")\n",
        "    print(f\"Created {total_pred_sequences} prediction sequences from {pred_valid_customers} customers\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(np.array(X_train_windows), dtype=torch.float32).unsqueeze(-1)\n",
        "    y_train_tensor = torch.tensor(np.array(y_train_windows), dtype=torch.float32)\n",
        "    X_pred_tensor = torch.tensor(np.array(X_pred_windows), dtype=torch.float32).unsqueeze(-1)\n",
        "    y_pred_tensor = torch.tensor(np.array(y_pred_windows), dtype=torch.float32)\n",
        "\n",
        "    train_customer_indices_tensor = torch.tensor(customer_indices_train, dtype=torch.long)\n",
        "    pred_customer_indices_tensor = torch.tensor(customer_indices_pred, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_tensor,\n",
        "        'y_train': y_train_tensor,\n",
        "        'X_pred': X_pred_tensor,\n",
        "        'y_pred': y_pred_tensor,\n",
        "        'train_customer_indices': train_customer_indices_tensor,\n",
        "        'pred_customer_indices': pred_customer_indices_tensor,\n",
        "        'customer_mapping': customer_to_idx,\n",
        "        'num_customers': len(customer_to_idx),\n",
        "        'customer_cohorts': customer_cohorts,\n",
        "        'window_size': window_size,\n",
        "        'training_input_dates': training_input_dates,\n",
        "        'training_target_dates': training_target_dates,\n",
        "        'prediction_input_dates': prediction_input_dates,\n",
        "        'prediction_target_dates': prediction_target_dates\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Architecture"
      ],
      "metadata": {
        "id": "w3TA3PA83hmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNEuUj0KisUO"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)  # Store on same device as model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(1), :].to(x.device)  # Move to correct device\n",
        "\n",
        "\n",
        "class SlidingWindowTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, num_heads,\n",
        "                output_dim, num_customers=None, dropout=0.1):\n",
        "        super(SlidingWindowTransformer, self).__init__()\n",
        "\n",
        "        # Standard components\n",
        "        self.input_projection = nn.Linear(input_dim, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
        "        self.customer_embedding = nn.Embedding(num_customers+1, embed_dim)\n",
        "\n",
        "        # Transaction event detector\n",
        "        self.transaction_detector = nn.Sequential(\n",
        "            nn.Linear(input_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softplus()  # Ensure non-negative predictions\n",
        "        )\n",
        "\n",
        "        # Scale factor for prediction (learnable)\n",
        "        self.log_output_scale = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x, customer_ids, time_indices=None):\n",
        "        # Ensure correct dimensions\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(-1)\n",
        "\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # Transaction mask - highlight where transactions occurred\n",
        "        transaction_mask = (x > 0).float()\n",
        "\n",
        "        # Customer embedding\n",
        "        cust_embed = self.customer_embedding(customer_ids).unsqueeze(1).expand(-1, seq_len, -1)\n",
        "\n",
        "        # Process input\n",
        "        x_embed = self.input_projection(x)\n",
        "        x_embed = self.pos_encoding(x_embed)\n",
        "\n",
        "        # Add transaction signal\n",
        "        trans_signal = self.transaction_detector(x) * transaction_mask\n",
        "        x_embed = x_embed + cust_embed + trans_signal * 0.5\n",
        "\n",
        "        # Add time information if available\n",
        "        if time_indices is not None:\n",
        "            # Handle time indices shape\n",
        "            if time_indices.dim() == 1:\n",
        "                if time_indices.size(0) != seq_len:\n",
        "                    time_indices = time_indices[:seq_len] if time_indices.size(0) > seq_len else torch.cat([\n",
        "                        time_indices,\n",
        "                        time_indices[-1].repeat(seq_len - time_indices.size(0))\n",
        "                    ])\n",
        "                time_indices = time_indices.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "            # Use week of year as a feature (1-52)\n",
        "            week_of_year = (time_indices % 52) + 1\n",
        "            week_embed = torch.zeros((batch_size, seq_len, x_embed.size(-1)), device=x_embed.device)\n",
        "\n",
        "            # Create simple encoding - add a small signal based on week of year\n",
        "            for i in range(batch_size):\n",
        "                for j in range(seq_len):\n",
        "                    week = week_of_year[i, j].item()\n",
        "                    # Add signal for holiday seasons (weeks 50-52)\n",
        "                    if week >= 50:\n",
        "                        week_embed[i, j] += 0.2\n",
        "                    # Add signal for mid-year (weeks 25-27)\n",
        "                    elif 25 <= week <= 27:\n",
        "                        week_embed[i, j] += 0.1\n",
        "\n",
        "            x_embed = x_embed + week_embed\n",
        "\n",
        "        # Apply transformer\n",
        "        x_embed = self.transformer_encoder(x_embed)\n",
        "\n",
        "        # Use the entire context for prediction\n",
        "        # (changed from using only the last token in original model)\n",
        "        final_hidden = x_embed.mean(dim=1)  # Average pooling over sequence length\n",
        "\n",
        "        # Generate and scale output\n",
        "        output = self.output_projection(final_hidden)\n",
        "        output = output * torch.exp(self.log_output_scale)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDses-SdwFc6"
      },
      "source": [
        "## Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaOejbkw1Avt"
      },
      "outputs": [],
      "source": [
        "def enhanced_sliding_window_train(model, train_loader, val_loader, num_epochs=50, patience=10,\n",
        "                        learning_rate=0.001, weight_decay=0.001, device=None):\n",
        "    \"\"\"\n",
        "    Enhanced training function with improvements for scale factor issues\n",
        "    and imbalanced data handling.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : nn.Module\n",
        "        Model to train\n",
        "    train_loader : DataLoader\n",
        "        DataLoader for training data\n",
        "    val_loader : DataLoader\n",
        "        DataLoader for validation data\n",
        "    num_epochs : int\n",
        "        Number of epochs to train\n",
        "    patience : int\n",
        "        Number of epochs to wait for improvement before early stopping\n",
        "    learning_rate : float\n",
        "        Learning rate for optimizer\n",
        "    weight_decay : float\n",
        "        Weight decay for optimizer\n",
        "    device : torch.device\n",
        "        Device to use for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : nn.Module\n",
        "        Trained model\n",
        "    dict\n",
        "        Training history\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer with lower initial learning rate\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Scheduler with more gradual warmup\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=0.005,  # Lower max learning rate\n",
        "        total_steps=num_epochs * len(train_loader),\n",
        "        pct_start=0.15  # Longer warmup\n",
        "    )\n",
        "\n",
        "    # For early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    # For mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_rmse': [],\n",
        "        'val_mae': [],\n",
        "        'scale_factors': [],\n",
        "        'transaction_rmse': []  # Track transaction-only RMSE\n",
        "    }\n",
        "\n",
        "    # Enhanced loss function with better balancing\n",
        "    def enhanced_transaction_loss(y_pred, y_true, alpha=3.0, beta=1.0, zero_penalty_weight=10.0):\n",
        "        \"\"\"Enhanced loss function with better handling of rare transactions\"\"\"\n",
        "        # Apply global scale initialization if needed (first 3 epochs)\n",
        "        global_scale_applied = False\n",
        "\n",
        "        # Scale correction - with reduced maximum to prevent extreme values\n",
        "        batch_scale_factor = torch.mean(y_true) / (torch.mean(y_pred) + 1e-8)\n",
        "        batch_scale_factor = torch.clamp(batch_scale_factor, 0.5, 500.0)  # Narrower range\n",
        "        y_pred_scaled = y_pred * batch_scale_factor\n",
        "\n",
        "        # Standard MSE with dynamic weighting based on target value\n",
        "        base_loss = F.mse_loss(y_pred_scaled, y_true, reduction='none')\n",
        "\n",
        "        # Find where actual transactions occurred\n",
        "        nonzero_mask = (y_true > 0)\n",
        "        zero_mask = ~nonzero_mask\n",
        "\n",
        "        # Apply higher weights to non-zero targets\n",
        "        weights = torch.ones_like(y_true, device=y_true.device)\n",
        "        if nonzero_mask.sum() > 0:\n",
        "            # Calculate proportion of zeros vs non-zeros for dynamic weighting\n",
        "            zero_ratio = zero_mask.sum().float() / nonzero_mask.sum().float()\n",
        "            # Cap the ratio to prevent extreme weights\n",
        "            zero_ratio = torch.clamp(zero_ratio, 1.0, 100.0)\n",
        "            weights[nonzero_mask] = alpha * zero_ratio\n",
        "\n",
        "        # Weighted MSE loss\n",
        "        weighted_mse = (base_loss * weights).mean()\n",
        "\n",
        "        # Transaction specific loss\n",
        "        if nonzero_mask.sum() > 0:\n",
        "            # Calculate loss for transactions\n",
        "            transaction_loss = F.mse_loss(y_pred_scaled[nonzero_mask], y_true[nonzero_mask])\n",
        "\n",
        "            # Additional penalty for predicting near-zero when there are transactions\n",
        "            zero_pred_mask = (y_pred_scaled[nonzero_mask] < 0.05)  # Increased threshold\n",
        "            if zero_pred_mask.sum() > 0:\n",
        "                zero_penalty = torch.mean((0.2 - y_pred_scaled[nonzero_mask][zero_pred_mask])**2)\n",
        "                # Increased penalty for zero predictions\n",
        "                zero_penalty = zero_penalty * zero_penalty_weight\n",
        "            else:\n",
        "                zero_penalty = torch.tensor(0.0, device=y_pred.device)\n",
        "        else:\n",
        "            transaction_loss = torch.tensor(0.0, device=y_pred.device)\n",
        "            zero_penalty = torch.tensor(0.0, device=y_pred.device)\n",
        "\n",
        "        # Volume consistency with higher weight\n",
        "        pred_total = y_pred_scaled.sum(dim=1)\n",
        "        true_total = y_true.sum(dim=1)\n",
        "        volume_penalty = beta * torch.mean(torch.abs(pred_total - true_total))\n",
        "\n",
        "        # Combined loss with improved weighting\n",
        "        return weighted_mse + transaction_loss + volume_penalty + zero_penalty\n",
        "\n",
        "    print(f\"Starting enhanced training on {device}...\")\n",
        "\n",
        "    # Track persistently high scale factors\n",
        "    high_scale_epochs = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        epoch_scale_factors = []\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for X_batch, y_batch, customer_batch in progress_bar:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            customer_batch = customer_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with autocast (mixed precision)\n",
        "            with autocast():\n",
        "                predictions = model(X_batch, customer_batch)\n",
        "\n",
        "                # Calculate batch scale factor for monitoring\n",
        "                batch_mean_true = torch.mean(y_batch).item()\n",
        "                batch_mean_pred = torch.mean(predictions).item()\n",
        "                if batch_mean_pred > 1e-10:\n",
        "                    curr_scale = batch_mean_true / batch_mean_pred\n",
        "                    curr_scale = min(max(curr_scale, 0.5), 500.0)\n",
        "                    epoch_scale_factors.append(curr_scale)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = enhanced_transaction_loss(predictions, y_batch)\n",
        "\n",
        "            # Backward pass with scaling\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Unscale before gradient clipping\n",
        "            scaler.unscale_(optimizer)\n",
        "            # Lower max norm to prevent large updates\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "            # Update weights with scaler\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Update scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Store average scale factor for this epoch\n",
        "        if epoch_scale_factors:\n",
        "            avg_scale_factor = np.mean(epoch_scale_factors)\n",
        "            history['scale_factors'].append(avg_scale_factor)\n",
        "\n",
        "            # Check for persistently high scale factors\n",
        "            if avg_scale_factor > 100 and epoch > 5:\n",
        "                high_scale_epochs += 1\n",
        "\n",
        "                # Apply direct scaling to output layer if scale factor remains high\n",
        "                if high_scale_epochs >= 3:\n",
        "                    print(f\"  Applying direct scale adjustment to output layer\")\n",
        "                    for name, param in model.named_parameters():\n",
        "                        if 'output' in name and 'weight' in name:\n",
        "                            # Scale up the output layer weights to increase predictions\n",
        "                            param.data *= 2.0\n",
        "                    high_scale_epochs = 0  # Reset counter after adjustment\n",
        "            else:\n",
        "                high_scale_epochs = 0  # Reset if scale factor improves\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_pred_list = []\n",
        "        val_target_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, customer_batch in val_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                customer_batch = customer_batch.to(device)\n",
        "\n",
        "                val_preds = model(X_batch, customer_batch)\n",
        "\n",
        "                # Apply the current average scale factor to predictions for metrics\n",
        "                if epoch_scale_factors:\n",
        "                    # Using a smoothed scale factor from training\n",
        "                    # *** Add a check to prevent NaN or inf in scale factor ***\n",
        "                    avg_scale_factor = np.nan_to_num(avg_scale_factor, nan=1.0, posinf=1.0, neginf=1.0)\n",
        "                    val_preds_scaled = val_preds * min(avg_scale_factor, 100)\n",
        "                else:\n",
        "                    val_preds_scaled = val_preds\n",
        "\n",
        "                # Calculate loss\n",
        "                batch_loss = enhanced_transaction_loss(val_preds, y_batch).item()\n",
        "                val_loss += batch_loss\n",
        "\n",
        "                # Store predictions and targets for metrics\n",
        "                val_pred_list.append(val_preds_scaled.cpu().numpy())\n",
        "                val_target_list.append(y_batch.cpu().numpy())\n",
        "\n",
        "        # Concatenate all batches\n",
        "        val_predictions = np.vstack(val_pred_list)\n",
        "        val_targets = np.vstack(val_target_list)\n",
        "\n",
        "        # Calculate metrics\n",
        "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
        "        val_rmse = np.sqrt(val_mse)\n",
        "        val_mae = mean_absolute_error(val_targets, val_predictions)\n",
        "\n",
        "        # Calculate transaction-only RMSE (for non-zero targets)\n",
        "        nonzero_mask = val_targets > 0\n",
        "        if nonzero_mask.sum() > 0:\n",
        "            tx_mse = mean_squared_error(\n",
        "                val_targets[nonzero_mask],\n",
        "                val_predictions[nonzero_mask]\n",
        "            )\n",
        "            tx_rmse = np.sqrt(tx_mse)\n",
        "        else:\n",
        "            tx_rmse = 0.0\n",
        "\n",
        "        # Average metrics\n",
        "        val_loss /= len(val_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_rmse'].append(val_rmse)\n",
        "        history['val_mae'].append(val_mae)\n",
        "        history['transaction_rmse'].append(tx_rmse)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "        print(f\"  Val RMSE: {val_rmse:.6f}, Val MAE: {val_mae:.6f}, Tx RMSE: {tx_rmse:.6f}\")\n",
        "        if epoch_scale_factors:\n",
        "            print(f\"  Avg Scale Factor: {avg_scale_factor:.2f}\")\n",
        "\n",
        "        # Check for improvement with a more stable metric combination\n",
        "        val_metric = val_loss + 0.1 * tx_rmse  # Combined metric that considers transaction accuracy\n",
        "        if val_metric < best_val_loss:\n",
        "            best_val_loss = val_metric\n",
        "            best_model = {k: v.cpu().detach() for k, v in model.state_dict().items()}\n",
        "            patience_counter = 0\n",
        "            print(f\"  New best model! Val Metric: {val_metric:.6f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnf4np-11GcW"
      },
      "source": [
        "## Model Estimation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKZPo3xeopvf"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on test data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : nn.Module\n",
        "        Trained model\n",
        "    test_loader : DataLoader\n",
        "        DataLoader for test data\n",
        "    device : torch.device\n",
        "        Device to use for evaluation\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Evaluation metrics\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_pred_list = []\n",
        "    test_target_list = []\n",
        "    test_customer_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, customer_batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            customer_batch = customer_batch.to(device)\n",
        "\n",
        "            test_preds = model(X_batch, customer_batch)\n",
        "\n",
        "            # Store predictions, targets, and customers\n",
        "            test_pred_list.append(test_preds.cpu().numpy())\n",
        "            test_target_list.append(y_batch.cpu().numpy())\n",
        "            test_customer_list.append(customer_batch.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    test_predictions = np.vstack(test_pred_list)\n",
        "    test_targets = np.vstack(test_target_list)\n",
        "    test_customers = np.concatenate(test_customer_list)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    test_mse = mean_squared_error(test_targets, test_predictions)\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_mae = mean_absolute_error(test_targets, test_predictions)\n",
        "\n",
        "    # Calculate customer-specific metrics\n",
        "    unique_customers = np.unique(test_customers)\n",
        "    customer_metrics = {}\n",
        "\n",
        "    for cust_id in unique_customers:\n",
        "        cust_mask = test_customers == cust_id\n",
        "        cust_preds = test_predictions[cust_mask]\n",
        "        cust_targets = test_targets[cust_mask]\n",
        "\n",
        "        if len(cust_preds) > 0:\n",
        "            cust_mse = mean_squared_error(cust_targets, cust_preds)\n",
        "            cust_rmse = np.sqrt(cust_mse)\n",
        "            cust_mae = mean_absolute_error(cust_targets, cust_preds)\n",
        "\n",
        "            customer_metrics[int(cust_id)] = {\n",
        "                'rmse': cust_rmse,\n",
        "                'mae': cust_mae,\n",
        "                'num_sequences': len(cust_preds)\n",
        "            }\n",
        "\n",
        "    # Transaction-specific metrics (non-zero values)\n",
        "    nonzero_mask = test_targets > 0\n",
        "    if nonzero_mask.sum() > 0:\n",
        "        trans_mse = mean_squared_error(test_targets[nonzero_mask], test_predictions[nonzero_mask])\n",
        "        trans_rmse = np.sqrt(trans_mse)\n",
        "        trans_mae = mean_absolute_error(test_targets[nonzero_mask], test_predictions[nonzero_mask])\n",
        "    else:\n",
        "        trans_rmse = 0\n",
        "        trans_mae = 0\n",
        "\n",
        "    print(f\"Test RMSE: {test_rmse:.6f}, Test MAE: {test_mae:.6f}\")\n",
        "    print(f\"Transaction-only RMSE: {trans_rmse:.6f}, MAE: {trans_mae:.6f}\")\n",
        "\n",
        "    return {\n",
        "        'overall': {\n",
        "            'mse': test_mse,\n",
        "            'rmse': test_rmse,\n",
        "            'mae': test_mae\n",
        "        },\n",
        "        'transaction_only': {\n",
        "            'rmse': trans_rmse,\n",
        "            'mae': trans_mae\n",
        "        },\n",
        "        'by_customer': customer_metrics,\n",
        "        'predictions': test_predictions,\n",
        "        'targets': test_targets,\n",
        "        'customers': test_customers\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4JeSLYP1HM8"
      },
      "outputs": [],
      "source": [
        "# After running the pipeline and training the model:\n",
        "\n",
        "# 1. Visualize training history\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss along with performance metrics.\n",
        "    \"\"\"\n",
        "    # Create figure with subplots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    axs[0].plot(history['train_loss'], label='Training Loss')\n",
        "    axs[0].plot(history['val_loss'], label='Validation Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].set_title('Training and Validation Loss')\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(alpha=0.3)\n",
        "\n",
        "    # Plot validation RMSE and MAE\n",
        "    if 'val_rmse' in history:\n",
        "        axs[1].plot(history['val_rmse'], label='RMSE')\n",
        "        axs[1].plot(history['val_mae'], label='MAE')\n",
        "        axs[1].set_xlabel('Epoch')\n",
        "        axs[1].set_ylabel('Error')\n",
        "        axs[1].set_title('Validation Metrics')\n",
        "        axs[1].legend()\n",
        "        axs[1].grid(alpha=0.3)\n",
        "\n",
        "    # Plot scale factors if available\n",
        "    if 'scale_factors' in history and history['scale_factors']:\n",
        "        axs[2].plot(history['scale_factors'])\n",
        "        axs[2].set_xlabel('Epoch')\n",
        "        axs[2].set_ylabel('Scale Factor')\n",
        "        axs[2].set_title('Scale Factor Evolution')\n",
        "        axs[2].grid(alpha=0.3)\n",
        "    else:\n",
        "        axs[2].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 2. Visualize predictions\n",
        "def visualize_predictions_fixed_timeframe(window_size, input_sequences, true_future, predictions,\n",
        "                                         num_samples=5, customer_ids=None, start_idx=0):\n",
        "    \"\"\"\n",
        "    Visualize input sequences, true future values, and predictions for selected samples\n",
        "    with fixed timeframe approach, ensuring samples from different customers.\n",
        "    \"\"\"\n",
        "    if customer_ids is None:\n",
        "        # If no customer IDs provided, use default selection logic\n",
        "        if start_idx + num_samples > len(input_sequences):\n",
        "            start_idx = max(0, len(input_sequences) - num_samples)\n",
        "            num_samples = min(num_samples, len(input_sequences) - start_idx)\n",
        "\n",
        "        indices = list(range(start_idx, start_idx + num_samples))\n",
        "    else:\n",
        "        # Find unique customer IDs\n",
        "        unique_customers = np.unique(customer_ids)\n",
        "\n",
        "        # Select one sample from each unique customer, up to num_samples\n",
        "        indices = []\n",
        "        for customer in unique_customers[:min(len(unique_customers), num_samples)]:\n",
        "            # Find indices for this customer\n",
        "            customer_indices = np.where(customer_ids == customer)[0]\n",
        "            if len(customer_indices) > 0:\n",
        "                # Take the first window for this customer\n",
        "                indices.append(customer_indices[0])\n",
        "\n",
        "        # If we need more samples to reach num_samples, add randomly\n",
        "        if len(indices) < num_samples:\n",
        "            remaining = num_samples - len(indices)\n",
        "            all_indices = np.arange(len(input_sequences))\n",
        "            available = np.setdiff1d(all_indices, indices)\n",
        "            if len(available) > 0:\n",
        "                additional = np.random.choice(available,\n",
        "                                             size=min(remaining, len(available)),\n",
        "                                             replace=False)\n",
        "                indices.extend(additional)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(15, 4 * len(indices)))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(len(indices), 1, i+1)\n",
        "\n",
        "        # Get data\n",
        "        input_seq = input_sequences[idx].flatten()\n",
        "        true_seq = true_future[idx].flatten()\n",
        "        pred_seq = predictions[idx].flatten()\n",
        "\n",
        "        # Time indices\n",
        "        input_weeks = np.arange(window_size)\n",
        "        forecast_weeks = np.arange(len(true_seq))\n",
        "\n",
        "        # Plot input sequence\n",
        "        plt.plot(input_weeks, input_seq, 'o-', color='blue',\n",
        "                 label='Input Sequence', alpha=0.7, markersize=3)\n",
        "\n",
        "        # Plot forecast window with offset\n",
        "        plt.plot(forecast_weeks + window_size, true_seq, 'o-', color='green',\n",
        "                 label='True Future', alpha=0.7, markersize=3)\n",
        "        plt.plot(forecast_weeks + window_size, pred_seq, 'x--', color='red',\n",
        "                 label='Prediction', alpha=0.7, markersize=4)\n",
        "\n",
        "        # Add vertical line to separate input and forecast\n",
        "        plt.axvline(x=window_size-1, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "        # Add customer ID if available\n",
        "        if customer_ids is not None:\n",
        "            # Check if customer_to_idx and idx_to_customer are available in global scope\n",
        "            if 'idx_to_customer' in globals():\n",
        "                original_id = idx_to_customer.get(customer_ids[idx], f\"Index {customer_ids[idx]}\")\n",
        "                title = f'Sample {i} (Original Customer ID: {original_id})'\n",
        "            else:\n",
        "                title = f'Sample {i} (Customer {customer_ids[idx]})'\n",
        "        else:\n",
        "            title = f'Sample {i}'\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.ylabel('Transactions')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('Transaction Sequence Predictions', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "    plt.show()\n",
        "\n",
        "# 3. Visualize aggregated results\n",
        "def visualize_aggregated_results_fixed_timeframe(window_size, input_sequences, true_future, predictions):\n",
        "    \"\"\"\n",
        "    Visualize aggregated predictions vs true values across all samples.\n",
        "    Shows average input, target, and predicted sequences over time.\n",
        "    \"\"\"\n",
        "    # Compute mean across samples\n",
        "    mean_input = np.mean(input_sequences, axis=0).flatten()\n",
        "    mean_true = np.mean(true_future, axis=0).flatten()\n",
        "    mean_pred = np.mean(predictions, axis=0).flatten()\n",
        "\n",
        "    # Time indices\n",
        "    input_weeks = np.arange(window_size)\n",
        "    forecast_weeks = np.arange(len(mean_true))\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(input_weeks, mean_input, 'o-', label='Mean Input Sequence', color='blue')\n",
        "    plt.plot(forecast_weeks + window_size, mean_true, 'o-', label='Mean True Future', color='green')\n",
        "    plt.plot(forecast_weeks + window_size, mean_pred, 'x--', label='Mean Prediction', color='red')\n",
        "\n",
        "    plt.axvline(x=window_size-1, color='gray', linestyle='--', alpha=0.5)\n",
        "    plt.title('Aggregated Transaction Predictions')\n",
        "    plt.xlabel('Week')\n",
        "    plt.ylabel('Average Transactions')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2TKN2_509lF"
      },
      "source": [
        "##Key Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8AhCb_vnIMq"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "df = pd.read_csv('15-transactions_allCohorts.csv')\n",
        "df['Date'] = pd.to_datetime(df['ORDER_DATE'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed time periods\n",
        "TRAINING_INPUT_PERIOD = ('2005-01-01', '2006-12-30')\n",
        "TRAINING_TARGET_PERIOD = ('2007-01-02', '2009-01-02')\n",
        "PREDICTION_INPUT_PERIOD = ('2007-01-01', '2009-01-01')\n",
        "PREDICTION_TARGET_PERIOD = ('2009-01-02', '2011-01-01')\n",
        "\n",
        "\n",
        "##10 cohort\n",
        "\n",
        "pipeline_data = fixed_timeframe_sliding_window_pipeline(\n",
        "    df,\n",
        "    training_input_period=TRAINING_INPUT_PERIOD,\n",
        "    training_target_period=TRAINING_TARGET_PERIOD,\n",
        "    prediction_input_period=PREDICTION_INPUT_PERIOD,\n",
        "    prediction_target_period=PREDICTION_TARGET_PERIOD,\n",
        "    window_size=16,\n",
        "    step_size=4,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 10),\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Use the pipeline data for model training\n",
        "sequence_data = pipeline_data['sequence_data']\n",
        "dataloaders = pipeline_data['dataloaders']"
      ],
      "metadata": {
        "id": "XYwVRnPQ4ilk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om9RPGFS1HWy"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': len(sequence_data['training_target_dates']),  # Output dimension is the target period length\n",
        "    'num_customers': sequence_data['num_customers'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "model = SlidingWindowTransformer(**model_params).to(device)\n",
        "\n",
        "# Train the model\n",
        "trained_model, history = enhanced_sliding_window_train(\n",
        "    model,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    num_epochs=50,\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate on prediction data\n",
        "pred_metrics = evaluate_model(\n",
        "    trained_model,\n",
        "    dataloaders['pred'],\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIZpXI-aSIi3"
      },
      "outputs": [],
      "source": [
        "# After training the model and evaluating on prediction data:\n",
        "\n",
        "# 1. Visualize training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Extract prediction data for visualization\n",
        "pred_data = []\n",
        "pred_targets = []\n",
        "pred_outputs = []\n",
        "pred_customer_ids = []\n",
        "\n",
        "# Extract a batch of data from the prediction dataloader\n",
        "for inputs, targets, customer_ids in dataloaders['pred']:\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = trained_model(inputs.to(device), customer_ids.to(device)).cpu()\n",
        "\n",
        "    # Store data for visualization\n",
        "    pred_data.append(inputs.cpu().numpy())\n",
        "    pred_targets.append(targets.cpu().numpy())\n",
        "    pred_outputs.append(outputs.numpy())\n",
        "    pred_customer_ids.append(customer_ids.numpy())\n",
        "\n",
        "    # Only process a few batches for visualization\n",
        "    if len(pred_data) >= 10:\n",
        "        break\n",
        "\n",
        "# Concatenate batches\n",
        "pred_data = np.vstack(pred_data)\n",
        "pred_targets = np.vstack(pred_targets)\n",
        "pred_outputs = np.vstack(pred_outputs)\n",
        "pred_customer_ids = np.concatenate(pred_customer_ids)\n",
        "\n",
        "# 3. Visualize individual predictions\n",
        "visualize_predictions_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs,\n",
        "    num_samples=5,\n",
        "    customer_ids=pred_customer_ids\n",
        ")\n",
        "\n",
        "# 4. Visualize aggregated results\n",
        "visualize_aggregated_results_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Equal Length Group\n",
        "\n",
        "From this section below, 2 other temporal settings are provided as devised in thesis."
      ],
      "metadata": {
        "id": "GFQcYx2TxWqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed time periods\n",
        "TRAINING_INPUT_PERIOD = ('2005-01-01', '2006-12-30')\n",
        "TRAINING_TARGET_PERIOD = ('2007-01-02', '2009-01-02')\n",
        "PREDICTION_INPUT_PERIOD = ('2007-01-01', '2009-01-01')\n",
        "PREDICTION_TARGET_PERIOD = ('2009-01-02', '2011-01-01')\n",
        "\n",
        "\n",
        "##15 cohort\n",
        "\n",
        "pipeline_data = fixed_timeframe_sliding_window_pipeline(\n",
        "    df,\n",
        "    training_input_period=TRAINING_INPUT_PERIOD,\n",
        "    training_target_period=TRAINING_TARGET_PERIOD,\n",
        "    prediction_input_period=PREDICTION_INPUT_PERIOD,\n",
        "    prediction_target_period=PREDICTION_TARGET_PERIOD,\n",
        "    window_size=16,\n",
        "    step_size=4,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(2, 16),\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Use the pipeline data for model training\n",
        "sequence_data = pipeline_data['sequence_data']\n",
        "dataloaders = pipeline_data['dataloaders']\n"
      ],
      "metadata": {
        "id": "8NI5A9UrxfKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP9Q4kt-yNP_"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': len(sequence_data['training_target_dates']),  # Output dimension is the target period length\n",
        "    'num_customers': sequence_data['num_customers'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "model = SlidingWindowTransformer(**model_params).to(device)\n",
        "\n",
        "# Train the model\n",
        "trained_model, history = enhanced_sliding_window_train(\n",
        "    model,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    num_epochs=50,\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate on prediction data\n",
        "pred_metrics = evaluate_model(\n",
        "    trained_model,\n",
        "    dataloaders['pred'],\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz4BNsO6yd8i"
      },
      "outputs": [],
      "source": [
        "# After training the model and evaluating on prediction data:\n",
        "\n",
        "# 1. Visualize training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Extract prediction data for visualization\n",
        "pred_data = []\n",
        "pred_targets = []\n",
        "pred_outputs = []\n",
        "pred_customer_ids = []\n",
        "\n",
        "# Extract a batch of data from the prediction dataloader\n",
        "for inputs, targets, customer_ids in dataloaders['pred']:\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = trained_model(inputs.to(device), customer_ids.to(device)).cpu()\n",
        "\n",
        "    # Store data for visualization\n",
        "    pred_data.append(inputs.cpu().numpy())\n",
        "    pred_targets.append(targets.cpu().numpy())\n",
        "    pred_outputs.append(outputs.numpy())\n",
        "    pred_customer_ids.append(customer_ids.numpy())\n",
        "\n",
        "    # Only process a few batches for visualization\n",
        "    if len(pred_data) >= 10:\n",
        "        break\n",
        "\n",
        "# Concatenate batches\n",
        "pred_data = np.vstack(pred_data)\n",
        "pred_targets = np.vstack(pred_targets)\n",
        "pred_outputs = np.vstack(pred_outputs)\n",
        "pred_customer_ids = np.concatenate(pred_customer_ids)\n",
        "\n",
        "# 3. Visualize individual predictions\n",
        "visualize_predictions_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs,\n",
        "    num_samples=5,\n",
        "    customer_ids=pred_customer_ids\n",
        ")\n",
        "\n",
        "# 4. Visualize aggregated results\n",
        "visualize_aggregated_results_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##20 cohort\n",
        "\n",
        "pipeline_data = fixed_timeframe_sliding_window_pipeline(\n",
        "    df,\n",
        "    training_input_period=TRAINING_INPUT_PERIOD,\n",
        "    training_target_period=TRAINING_TARGET_PERIOD,\n",
        "    prediction_input_period=PREDICTION_INPUT_PERIOD,\n",
        "    prediction_target_period=PREDICTION_TARGET_PERIOD,\n",
        "    window_size=16,\n",
        "    step_size=4,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 20),\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Use the pipeline data for model training\n",
        "sequence_data = pipeline_data['sequence_data']\n",
        "dataloaders = pipeline_data['dataloaders']\n"
      ],
      "metadata": {
        "id": "fcr-tm-xynkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': len(sequence_data['training_target_dates']),  # Output dimension is the target period length\n",
        "    'num_customers': sequence_data['num_customers'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "model = SlidingWindowTransformer(**model_params).to(device)\n",
        "\n",
        "# Train the model\n",
        "trained_model, history = enhanced_sliding_window_train(\n",
        "    model,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    num_epochs=50,\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate on prediction data\n",
        "pred_metrics = evaluate_model(\n",
        "    trained_model,\n",
        "    dataloaders['pred'],\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "Exr4bpXtytIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the model and evaluating on prediction data:\n",
        "\n",
        "# 1. Visualize training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Extract prediction data for visualization\n",
        "pred_data = []\n",
        "pred_targets = []\n",
        "pred_outputs = []\n",
        "pred_customer_ids = []\n",
        "\n",
        "# Extract a batch of data from the prediction dataloader\n",
        "for inputs, targets, customer_ids in dataloaders['pred']:\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = trained_model(inputs.to(device), customer_ids.to(device)).cpu()\n",
        "\n",
        "    # Store data for visualization\n",
        "    pred_data.append(inputs.cpu().numpy())\n",
        "    pred_targets.append(targets.cpu().numpy())\n",
        "    pred_outputs.append(outputs.numpy())\n",
        "    pred_customer_ids.append(customer_ids.numpy())\n",
        "\n",
        "    # Only process a few batches for visualization\n",
        "    if len(pred_data) >= 10:\n",
        "        break\n",
        "\n",
        "# Concatenate batches\n",
        "pred_data = np.vstack(pred_data)\n",
        "pred_targets = np.vstack(pred_targets)\n",
        "pred_outputs = np.vstack(pred_outputs)\n",
        "pred_customer_ids = np.concatenate(pred_customer_ids)\n",
        "\n",
        "# 3. Visualize individual predictions\n",
        "visualize_predictions_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs,\n",
        "    num_samples=5,\n",
        "    customer_ids=pred_customer_ids\n",
        ")\n",
        "\n",
        "# 4. Visualize aggregated results\n",
        "visualize_aggregated_results_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs\n",
        ")"
      ],
      "metadata": {
        "id": "KZbLwsXvywq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Long Output Group"
      ],
      "metadata": {
        "id": "AeT2Z04Gy3Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fixed time periods\n",
        "TRAINING_INPUT_PERIOD = ('2005-01-01', '2006-12-30')\n",
        "TRAINING_TARGET_PERIOD = ('2007-01-02', '2009-12-31')\n",
        "PREDICTION_INPUT_PERIOD = ('2006-01-01', '2008-01-01')\n",
        "PREDICTION_TARGET_PERIOD = ('2008-01-02', '2011-01-01')"
      ],
      "metadata": {
        "id": "Nyqaj1bPzCvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calculate how many weeks for each period\n",
        "\n",
        "# Calculate the number of weeks for each period\n",
        "def calculate_weeks(start_date_str, end_date_str):\n",
        "    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d').date()\n",
        "    end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d').date()\n",
        "    delta = end_date - start_date\n",
        "    weeks = delta.days // 7\n",
        "    return weeks\n",
        "\n",
        "# Example usage with your defined periods:\n",
        "training_input_weeks = calculate_weeks(TRAINING_INPUT_PERIOD[0], TRAINING_INPUT_PERIOD[1])\n",
        "training_target_weeks = calculate_weeks(TRAINING_TARGET_PERIOD[0], TRAINING_TARGET_PERIOD[1])\n",
        "prediction_input_weeks = calculate_weeks(PREDICTION_INPUT_PERIOD[0], PREDICTION_INPUT_PERIOD[1])\n",
        "prediction_target_weeks = calculate_weeks(PREDICTION_TARGET_PERIOD[0], PREDICTION_TARGET_PERIOD[1])\n",
        "\n",
        "print(f\"Training Input Weeks: {training_input_weeks}\")\n",
        "print(f\"Training Target Weeks: {training_target_weeks}\")\n",
        "print(f\"Prediction Input Weeks: {prediction_input_weeks}\")\n",
        "print(f\"Prediction Target Weeks: {prediction_target_weeks}\")\n"
      ],
      "metadata": {
        "id": "vzUjRoXnzkys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cohort 10\n",
        "\n",
        "# Run the pipeline\n",
        "pipeline_data = fixed_timeframe_sliding_window_pipeline(\n",
        "    df,\n",
        "    training_input_period=TRAINING_INPUT_PERIOD,\n",
        "    training_target_period=TRAINING_TARGET_PERIOD,\n",
        "    prediction_input_period=PREDICTION_INPUT_PERIOD,\n",
        "    prediction_target_period=PREDICTION_TARGET_PERIOD,\n",
        "    window_size=16,\n",
        "    step_size=4,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 10),\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Use the pipeline data for model training\n",
        "sequence_data = pipeline_data['sequence_data']\n",
        "dataloaders = pipeline_data['dataloaders']\n"
      ],
      "metadata": {
        "id": "c9VfgWkhzv35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': len(sequence_data['training_target_dates']),  # Output dimension is the target period length\n",
        "    'num_customers': sequence_data['num_customers'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "model = SlidingWindowTransformer(**model_params).to(device)\n",
        "\n",
        "# Train the model\n",
        "trained_model, history = enhanced_sliding_window_train(\n",
        "    model,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    num_epochs=50,\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate on prediction data\n",
        "pred_metrics = evaluate_model(\n",
        "    trained_model,\n",
        "    dataloaders['pred'],\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "5ZoXmilk0Fd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the model and evaluating on prediction data:\n",
        "\n",
        "# 1. Visualize training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Extract prediction data for visualization\n",
        "pred_data = []\n",
        "pred_targets = []\n",
        "pred_outputs = []\n",
        "pred_customer_ids = []\n",
        "\n",
        "# Extract a batch of data from the prediction dataloader\n",
        "for inputs, targets, customer_ids in dataloaders['pred']:\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = trained_model(inputs.to(device), customer_ids.to(device)).cpu()\n",
        "\n",
        "    # Store data for visualization\n",
        "    pred_data.append(inputs.cpu().numpy())\n",
        "    pred_targets.append(targets.cpu().numpy())\n",
        "    pred_outputs.append(outputs.numpy())\n",
        "    pred_customer_ids.append(customer_ids.numpy())\n",
        "\n",
        "    # Only process a few batches for visualization\n",
        "    if len(pred_data) >= 10:\n",
        "        break\n",
        "\n",
        "# Concatenate batches\n",
        "pred_data = np.vstack(pred_data)\n",
        "pred_targets = np.vstack(pred_targets)\n",
        "pred_outputs = np.vstack(pred_outputs)\n",
        "pred_customer_ids = np.concatenate(pred_customer_ids)\n",
        "\n",
        "# 3. Visualize individual predictions\n",
        "visualize_predictions_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs,\n",
        "    num_samples=5,\n",
        "    customer_ids=pred_customer_ids\n",
        ")\n",
        "\n",
        "# 4. Visualize aggregated results\n",
        "visualize_aggregated_results_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs\n",
        ")"
      ],
      "metadata": {
        "id": "ROHPi4WI0GGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PWRPtDsZ0Ksv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cohort 15\n",
        "\n",
        "# Run the pipeline\n",
        "pipeline_data = fixed_timeframe_sliding_window_pipeline(\n",
        "    df,\n",
        "    training_input_period=TRAINING_INPUT_PERIOD,\n",
        "    training_target_period=TRAINING_TARGET_PERIOD,\n",
        "    prediction_input_period=PREDICTION_INPUT_PERIOD,\n",
        "    prediction_target_period=PREDICTION_TARGET_PERIOD,\n",
        "    window_size=16,\n",
        "    step_size=4,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 15),\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Use the pipeline data for model training\n",
        "sequence_data = pipeline_data['sequence_data']\n",
        "dataloaders = pipeline_data['dataloaders']\n"
      ],
      "metadata": {
        "id": "yK0cHrjI0K0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': len(sequence_data['training_target_dates']),  # Output dimension is the target period length\n",
        "    'num_customers': sequence_data['num_customers'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "model = SlidingWindowTransformer(**model_params).to(device)\n",
        "\n",
        "# Train the model\n",
        "trained_model, history = enhanced_sliding_window_train(\n",
        "    model,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    num_epochs=50,\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate on prediction data\n",
        "pred_metrics = evaluate_model(\n",
        "    trained_model,\n",
        "    dataloaders['pred'],\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "lsQgQ8gb0T4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the model and evaluating on prediction data:\n",
        "\n",
        "# 1. Visualize training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Extract prediction data for visualization\n",
        "pred_data = []\n",
        "pred_targets = []\n",
        "pred_outputs = []\n",
        "pred_customer_ids = []\n",
        "\n",
        "# Extract a batch of data from the prediction dataloader\n",
        "for inputs, targets, customer_ids in dataloaders['pred']:\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = trained_model(inputs.to(device), customer_ids.to(device)).cpu()\n",
        "\n",
        "    # Store data for visualization\n",
        "    pred_data.append(inputs.cpu().numpy())\n",
        "    pred_targets.append(targets.cpu().numpy())\n",
        "    pred_outputs.append(outputs.numpy())\n",
        "    pred_customer_ids.append(customer_ids.numpy())\n",
        "\n",
        "    # Only process a few batches for visualization\n",
        "    if len(pred_data) >= 10:\n",
        "        break\n",
        "\n",
        "# Concatenate batches\n",
        "pred_data = np.vstack(pred_data)\n",
        "pred_targets = np.vstack(pred_targets)\n",
        "pred_outputs = np.vstack(pred_outputs)\n",
        "pred_customer_ids = np.concatenate(pred_customer_ids)\n",
        "\n",
        "# 3. Visualize individual predictions\n",
        "visualize_predictions_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs,\n",
        "    num_samples=5,\n",
        "    customer_ids=pred_customer_ids\n",
        ")\n",
        "\n",
        "# 4. Visualize aggregated results\n",
        "visualize_aggregated_results_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs\n",
        ")"
      ],
      "metadata": {
        "id": "CJsEKavM0VSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjojVtIR0ZmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cohort 20\n",
        "\n",
        "# Run the pipeline\n",
        "pipeline_data = fixed_timeframe_sliding_window_pipeline(\n",
        "    df,\n",
        "    training_input_period=TRAINING_INPUT_PERIOD,\n",
        "    training_target_period=TRAINING_TARGET_PERIOD,\n",
        "    prediction_input_period=PREDICTION_INPUT_PERIOD,\n",
        "    prediction_target_period=PREDICTION_TARGET_PERIOD,\n",
        "    window_size=16,\n",
        "    step_size=4,\n",
        "    customer_field='CUSTNO',\n",
        "    date_field='Date',\n",
        "    cohort_field='COHORT_NUMBER',\n",
        "    cohort_range=(1, 20),\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Use the pipeline data for model training\n",
        "sequence_data = pipeline_data['sequence_data']\n",
        "dataloaders = pipeline_data['dataloaders']\n"
      ],
      "metadata": {
        "id": "Gf0Z9WxQ0ZqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model_params = {\n",
        "    'input_dim': 1,\n",
        "    'embed_dim': 128,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 2,\n",
        "    'num_heads': 8,\n",
        "    'output_dim': len(sequence_data['training_target_dates']),  # Output dimension is the target period length\n",
        "    'num_customers': sequence_data['num_customers'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "model = SlidingWindowTransformer(**model_params).to(device)\n",
        "\n",
        "# Train the model\n",
        "trained_model, history = enhanced_sliding_window_train(\n",
        "    model,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    num_epochs=50,\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate on prediction data\n",
        "pred_metrics = evaluate_model(\n",
        "    trained_model,\n",
        "    dataloaders['pred'],\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "Soqnt2J60hIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the model and evaluating on prediction data:\n",
        "\n",
        "# 1. Visualize training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Extract prediction data for visualization\n",
        "pred_data = []\n",
        "pred_targets = []\n",
        "pred_outputs = []\n",
        "pred_customer_ids = []\n",
        "\n",
        "# Extract a batch of data from the prediction dataloader\n",
        "for inputs, targets, customer_ids in dataloaders['pred']:\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = trained_model(inputs.to(device), customer_ids.to(device)).cpu()\n",
        "\n",
        "    # Store data for visualization\n",
        "    pred_data.append(inputs.cpu().numpy())\n",
        "    pred_targets.append(targets.cpu().numpy())\n",
        "    pred_outputs.append(outputs.numpy())\n",
        "    pred_customer_ids.append(customer_ids.numpy())\n",
        "\n",
        "    # Only process a few batches for visualization\n",
        "    if len(pred_data) >= 10:\n",
        "        break\n",
        "\n",
        "# Concatenate batches\n",
        "pred_data = np.vstack(pred_data)\n",
        "pred_targets = np.vstack(pred_targets)\n",
        "pred_outputs = np.vstack(pred_outputs)\n",
        "pred_customer_ids = np.concatenate(pred_customer_ids)\n",
        "\n",
        "# 3. Visualize individual predictions\n",
        "visualize_predictions_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs,\n",
        "    num_samples=5,\n",
        "    customer_ids=pred_customer_ids\n",
        ")\n",
        "\n",
        "# 4. Visualize aggregated results\n",
        "visualize_aggregated_results_fixed_timeframe(\n",
        "    window_size=pipeline_data['config']['window_size'],\n",
        "    input_sequences=pred_data,\n",
        "    true_future=pred_targets,\n",
        "    predictions=pred_outputs\n",
        ")"
      ],
      "metadata": {
        "id": "hF2KlXUH0ijW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GUXgHczVpRSz"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
